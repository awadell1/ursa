{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Universal Research and Scientific Agent (URSA)","text":"<p>The flexible agentic workflow for accelerating scientific tasks. Composes information flow between agents for planning, code writing and execution, and online research to solve complex problems.</p>"},{"location":"arxiv_agent/","title":"ArxivAgent Documentation","text":"<p><code>ArxivAgent</code> is a class that helps fetch, process, and summarize scientific papers from arXiv. It uses LLMs to generate summaries of papers relevant to a given query and context.</p>"},{"location":"arxiv_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import ArxivAgent\n\n# Initialize the agent\nagent = ArxivAgent()\n\n# Run a query\nresult = agent.invoke(\n    arxiv_search_query=\"Experimental Constraints on neutron star radius\", \n    context=\"What are the constraints on the neutron star radius and what uncertainties are there on the constraints?\"\n)\n\n# Print the summary\nprint(result)\n</code></pre>"},{"location":"arxiv_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>ArxivAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> str \"openai/o3-mini\" The LLM model to use for summarization <code>summarize</code> bool True Whether to summarize the papers or just fetch them <code>process_images</code> bool True Whether to extract and describe images from papers <code>max_results</code> int 3 Maximum number of papers to fetch from arXiv <code>database_path</code> str 'arxiv_papers' Directory to store downloaded PDFs <code>summaries_path</code> str 'arxiv_generated_summaries' Directory to store paper summaries <code>vectorstore_path</code> str 'arxiv_vectorstores' Directory to store vector embeddings <code>download_papers</code> bool True Whether to download papers or use existing ones"},{"location":"arxiv_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"arxiv_agent/#customizing-the-agent","title":"Customizing the Agent","text":"<pre><code>agent = ArxivAgent(\n    llm=\"openai/o3\",  # Use a more powerful model\n    max_results=5,       # Fetch more papers\n    process_images=False,  # Skip image processing to save time\n    download_papers=False  # Use only papers already in database_path\n)\n</code></pre>"},{"location":"arxiv_agent/#running-multiple-queries","title":"Running Multiple Queries","text":"<pre><code># First query\nresult1 = agent.invoke(\n    arxiv_search_query=\"quantum computing error correction\", \n    context=\"Summarize recent advances in quantum error correction techniques\"\n)\n\n# Second query (will reuse downloaded papers if applicable)\nresult2 = agent.invoke(\n    arxiv_search_query=\"quantum computing algorithms\", \n    context=\"What are the most promising quantum algorithms for near-term devices?\"\n)\n</code></pre>"},{"location":"arxiv_agent/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Fetching Papers: The agent searches arXiv for papers matching your query and downloads them as PDFs.</p> </li> <li> <p>Processing: If <code>summarize=True</code>, each paper is:</p> </li> <li>Converted to text</li> <li>Split into chunks</li> <li>Embedded into a vector database</li> <li> <p>If <code>process_images=True</code>, images are extracted and described using GPT-4 Vision</p> </li> <li> <p>Summarization: The agent:</p> </li> <li>Retrieves the most relevant chunks based on your context</li> <li>Generates a summary for each paper</li> <li> <p>Creates a final summary addressing your specific context</p> </li> <li> <p>Output: Returns a comprehensive summary that synthesizes information from all relevant papers.</p> </li> </ol>"},{"location":"arxiv_agent/#notes","title":"Notes","text":"<ul> <li>Summaries and vector stores are cached, making subsequent queries faster.</li> <li>The agent uses a ThreadPoolExecutor to process papers in parallel.</li> <li>You can find the combined summaries in 'summaries_combined.txt' and the final summary in 'final_summary.txt'.</li> </ul>"},{"location":"chatollama_setup/","title":"Running with ChatOllama","text":"<p>Disable OPENAI_API_KEY by setting the following two env variales: (without both of these env vars ollama complains about auth)</p> <pre><code>$ export OPENAI_API_KEY=ollama\n$ export OPENAI_BASE_URL=&lt;YOUR OLLAMA ENDPOINT&gt;\n</code></pre> <p>Example Auth Error</p> <pre><code>openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: ollama. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n</code></pre>"},{"location":"combining_arxiv_and_execution/","title":"Using ArxivAgent and ExecutionAgent Together","text":"<p>This guide demonstrates how to combine the <code>ArxivAgent</code> and <code>ExecutionAgent</code> for comprehensive research and analysis workflows.</p>"},{"location":"combining_arxiv_and_execution/#overview","title":"Overview","text":"<p>This workflow enables you to: 1. Search and analyze papers from arXiv 2. Process the research findings 3. Generate executable code to analyze and visualize the data</p>"},{"location":"combining_arxiv_and_execution/#basic-usage","title":"Basic Usage","text":"<pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_litellm import ChatLiteLLM\nfrom ursa.agents import ArxivAgent, ExecutionAgent\n\n# Initialize the language model\nmodel = ChatLiteLLM(\n    model=\"openai/o3\",\n    max_tokens=50000,\n)\n\n# Initialize the ArxivAgent\narxiv_agent = ArxivAgent(\n    llm=model,\n    summarize=True,\n    process_images=False,\n    max_results=20,\n    database_path=\"arxiv_papers_materials1\",\n    summaries_path=\"arxiv_summaries_materials1\",\n    vectorstore_path=\"arxiv_vectorstores_materials1\",\n    download_papers=True,\n)\n\n# Run a search and analysis\nresearch_results = arxiv_agent.invoke(\n    arxiv_search_query=\"high entropy alloy hardness\",\n    context=\"What data and uncertainties are reported for hardness of the high entropy alloy and how that that compare to other alloys?\",\n)\n\n# Initialize the ExecutionAgent\nexecutor = ExecutionAgent(llm=model)\n\n# Create a task for the ExecutionAgent\nexecution_plan = f\"\"\"\nThe following is the summaries of research papers on the high entropy alloy hardness: \n{research_results}\n\nSummarize the results in a markdown document. Include a plot of the data extracted from the papers. This \nwill be reviewed by experts in the field so technical accuracy and clarity is critical.\n\"\"\"\n\n# Prepare input for the ExecutionAgent\ninit = {\"messages\": [HumanMessage(content=execution_plan)]}\n\n# Execute the plan\nfinal_results = executor.invoke(init)\n\n# Display results\nfor message in final_results[\"messages\"]:\n    print(message.content)\n</code></pre>"},{"location":"combining_arxiv_and_execution/#parameters","title":"Parameters","text":""},{"location":"combining_arxiv_and_execution/#arxivagent","title":"ArxivAgent","text":"Parameter Description <code>llm</code> Language model to use for analysis <code>summarize</code> Whether to summarize papers (boolean) <code>process_images</code> Whether to process images in papers (boolean) <code>max_results</code> Maximum number of papers to retrieve <code>database_path</code> Path to store downloaded papers <code>summaries_path</code> Path to store paper summaries <code>vectorstore_path</code> Path to store vector embeddings <code>download_papers</code> Whether to download full papers (boolean)"},{"location":"combining_arxiv_and_execution/#executionagent","title":"ExecutionAgent","text":"Parameter Description <code>llm</code> Language model to use for code generation and execution"},{"location":"combining_arxiv_and_execution/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Research Phase: ArxivAgent searches arXiv for relevant papers based on your query, downloads them, and analyzes their content according to your context.</p> </li> <li> <p>Analysis Phase: ExecutionAgent takes the research results and generates code to analyze and visualize the data.</p> </li> <li> <p>Output: The ExecutionAgent produces a markdown document with analysis, visualizations, and insights from the research.</p> </li> </ol>"},{"location":"combining_arxiv_and_execution/#use-cases","title":"Use Cases","text":"<ul> <li>Literature reviews on scientific topics</li> <li>Data extraction and visualization from research papers</li> <li>Comparative analysis across multiple publications</li> <li>Technical report generation</li> </ul>"},{"location":"combining_arxiv_and_execution/#notes","title":"Notes","text":"<ul> <li>Ensure you have sufficient disk space for paper storage</li> <li>Processing a large number of papers may take significant time</li> <li>The quality of analysis depends on the capabilities of the chosen language model</li> </ul>"},{"location":"combining_arxiv_and_execution_neutronStar/","title":"Using ArxivAgent and ExecutionAgent for Astrophysics Research","text":"<p>This guide demonstrates how to use the <code>ArxivAgent</code> and <code>ExecutionAgent</code> together to research neutron star properties and generate a comprehensive analysis.</p>"},{"location":"combining_arxiv_and_execution_neutronStar/#overview","title":"Overview","text":"<p>This workflow allows you to: 1. Search arXiv for papers on neutron star radius constraints 2. Process and summarize the research findings 3. Generate a markdown document with data visualization</p>"},{"location":"combining_arxiv_and_execution_neutronStar/#basic-usage","title":"Basic Usage","text":"<pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_litellm import ChatLiteLLM\nfrom ursa.agents import ArxivAgent, ExecutionAgent\n\n# Initialize the language model\nmodel = ChatLiteLLM(\n    model=\"openai/o3\",\n    max_tokens=50000,\n)\n\n# Initialize the ArxivAgent\narxiv_agent = ArxivAgent(\n    llm=model,\n    summarize=True,\n    process_images=False,\n    max_results=5,\n    database_path=\"arxiv_papers_neutron_star\",\n    summaries_path=\"arxiv_summaries_neutron_star\",\n    vectorstore_path=\"arxiv_vectorstores_neutron_star\",\n    download_papers=True,\n)\n\n# Run a search on neutron star radius constraints\nresearch_results = arxiv_agent.invoke(\n    arxiv_search_query=\"Experimental Constraints on neutron star radius\",\n    context=\"What are the constraints on the neutron star radius and what uncertainties are there on the constraints?\",\n)\n\n# Initialize the ExecutionAgent\nexecutor = ExecutionAgent(llm=model)\n\n# Create a task for the ExecutionAgent\nexecution_plan = f\"\"\"\nThe following is the summaries of research papers on the contraints on neutron\nstar radius: \n{research_results}\n\nSummarize the results in a markdown document. Include a plot of the data extracted from the papers. This \nwill be reviewed by experts in the field so technical accuracy and clarity is \ncritical.\n\"\"\"\n\n# Prepare input for the ExecutionAgent\ninit = {\"messages\": [HumanMessage(content=execution_plan)]}\n\n# Execute the plan\nfinal_results = executor.invoke(init)\n\n# Display results\nfor message in final_results[\"messages\"]:\n    print(message.content)\n</code></pre>"},{"location":"combining_arxiv_and_execution_neutronStar/#parameters","title":"Parameters","text":""},{"location":"combining_arxiv_and_execution_neutronStar/#arxivagent","title":"ArxivAgent","text":"Parameter Description <code>llm</code> Language model to use for analysis <code>summarize</code> Whether to summarize papers (boolean) <code>process_images</code> Whether to process images in papers (boolean) <code>max_results</code> Maximum number of papers to retrieve (5 in this example) <code>database_path</code> Path to store downloaded papers <code>summaries_path</code> Path to store paper summaries <code>vectorstore_path</code> Path to store vector embeddings <code>download_papers</code> Whether to download full papers (boolean)"},{"location":"combining_arxiv_and_execution_neutronStar/#executionagent","title":"ExecutionAgent","text":"Parameter Description <code>llm</code> Language model to use for code generation and execution"},{"location":"combining_arxiv_and_execution_neutronStar/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Research Phase: ArxivAgent searches arXiv for papers on neutron star radius constraints, downloads them, and analyzes their content.</p> </li> <li> <p>Analysis Phase: ExecutionAgent processes the research summaries and generates code to visualize the constraints and uncertainties.</p> </li> <li> <p>Output: The ExecutionAgent produces a markdown document with analysis, visualizations, and insights about neutron star radius constraints.</p> </li> </ol>"},{"location":"combining_arxiv_and_execution_neutronStar/#use-cases","title":"Use Cases","text":"<ul> <li>Astrophysics literature reviews</li> <li>Compilation of experimental constraints on astronomical objects</li> <li>Visualization of scientific data from multiple sources</li> <li>Creation of technical reports for expert audiences</li> </ul>"},{"location":"combining_arxiv_and_execution_neutronStar/#notes","title":"Notes","text":"<ul> <li>The quality of analysis depends on the available papers and the capabilities of the language model</li> <li>Consider adjusting <code>max_results</code> based on the breadth of research needed</li> <li>Ensure proper storage paths are set to avoid conflicts with other research projects</li> </ul>"},{"location":"execution_agent/","title":"ExecutionAgent Documentation","text":"<p><code>ExecutionAgent</code> is a class that enables AI-powered code execution, writing, and editing. It uses a state machine architecture to safely execute commands, write code files, and search for information.</p>"},{"location":"execution_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import ExecutionAgent\n\n# Initialize the agent\nagent = ExecutionAgent()\n\n# Run a prompt\nresult = agent.invoke(\"Write and execute a python script to print the first 10 integers.\")\n\n# Access the final response\nprint(result[\"messages\"][-1].content)\n</code></pre>"},{"location":"execution_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>ExecutionAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> str or BaseChatModel \"openai/gpt-4o-mini\" The LLM model to use <code>**kwargs</code> dict {} Additional parameters passed to the base agent"},{"location":"execution_agent/#features","title":"Features","text":""},{"location":"execution_agent/#code-execution","title":"Code Execution","text":"<p>The agent can safely execute shell commands in a controlled environment:</p> <pre><code>result = agent.invoke(\"Install numpy and create a script that uses it to calculate the mean of [1, 2, 3, 4, 5]\")\n</code></pre>"},{"location":"execution_agent/#code-writing","title":"Code Writing","text":"<p>The agent can write code files to a workspace directory:</p> <pre><code>result = agent.invoke(\"Create a Flask web application that displays 'Hello World'\")\n</code></pre>"},{"location":"execution_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"execution_agent/#customizing-the-workspace","title":"Customizing the Workspace","text":"<p>The agent creates a workspace folder with a randomly generated name for each run. You can access this workspace path from the result:</p> <pre><code>result = agent.invoke(\"Create a Python script\")\nworkspace_path = result[\"workspace\"]\nprint(f\"Files were created in: {workspace_path}\")\n</code></pre>"},{"location":"execution_agent/#setting-a-recursion-limit","title":"Setting a Recursion Limit","text":"<p>For complex tasks, you may need to adjust the recursion limit:</p> <pre><code>result = agent.invoke(\n    \"Create a complex project with multiple files and tests\", \n    recursion_limit=2000\n)\n</code></pre>"},{"location":"execution_agent/#safety-features","title":"Safety Features","text":"<p>The agent includes built-in safety checks for shell commands:</p> <ol> <li>Commands are evaluated for safety before execution</li> <li>Unsafe commands are blocked with explanations</li> <li>The agent suggests safer alternatives when appropriate</li> </ol>"},{"location":"execution_agent/#how-it-works","title":"How It Works","text":"<ol> <li>State Machine: The agent uses a directed graph to manage its workflow:</li> <li><code>agent</code> node: Processes user requests and generates responses</li> <li><code>safety_check</code> node: Evaluates command safety</li> <li><code>action</code> node: Executes tools (run_cmd, write_code, edit_code, search)</li> <li> <p><code>summarize</code> node: Creates a final summary when complete</p> </li> <li> <p>Tools:</p> </li> <li><code>run_cmd</code>: Executes shell commands in the workspace directory</li> <li><code>write_code</code>: Creates new code files with syntax highlighting</li> <li><code>edit_code</code>: Modifies existing code files with diff preview</li> <li> <p><code>search_tool</code>: Performs web searches via DuckDuckGo</p> </li> <li> <p>Visualization:</p> </li> <li>Code changes are displayed with syntax highlighting</li> <li>File edits show detailed diffs</li> <li>Command execution shows stdout and stderr</li> </ol>"},{"location":"execution_agent/#notes","title":"Notes","text":"<ul> <li>The agent creates a new workspace directory for each run</li> <li>Files are written to and executed from this workspace</li> <li>Shell commands have a 60000-second timeout by default</li> <li>The agent can handle keyboard interrupts during command execution</li> </ul>"},{"location":"humanInTheLoop_example/","title":"URSA Human-in-the-Loop Agent Interface Documentation","text":""},{"location":"humanInTheLoop_example/#overview","title":"Overview","text":"<p>This module implements a human-in-the-loop (HITL) interface for the URSA agent framework, allowing users to directly interact with different specialized AI agents through a command-line interface. The system maintains context between agent interactions and provides persistent storage for agent states.</p> <p>The file can be run with:</p> <p><code>python /path/to/hitl_basic.py</code></p>"},{"location":"humanInTheLoop_example/#setup-and-initialization","title":"Setup and Initialization","text":"<ol> <li>Creates a workspace directory for storing agent data and checkpoints</li> <li>Initializes SQLite databases and checkpointers for various agents:</li> <li>Executor agent</li> <li>Planner agent</li> <li>WebSearcher agent</li> <li>Configures the language model and embedding model</li> <li>Instantiates the following agents:</li> <li>ArxivAgent</li> <li>ExecutionAgent</li> <li>PlanningAgent</li> <li>WebSearchAgent</li> <li>RecallAgent</li> </ol>"},{"location":"humanInTheLoop_example/#user-interaction-loop","title":"User Interaction Loop","text":"<p>The function runs a continuous interaction loop until the user enters \"[USER DONE]\":</p> <ol> <li>Displays a header explaining how to use the interface</li> <li>Prompts the user for input</li> <li>Parses the input to determine which agent to invoke:</li> <li><code>[Arxiver]</code>: Searches ArXiv for academic papers</li> <li><code>[Executor]</code>: Executes code or commands</li> <li><code>[Planner]</code>: Creates plans or strategies</li> <li><code>[WebSearcher]</code>: Performs web searches</li> <li><code>[Rememberer]</code>: Retrieves information from memory</li> <li><code>[Chatter]</code>: Has a general conversation using the language model</li> <li>Importantly, the output from the previous agent interaction is automatically included in the prompt to the next agent. This creates a continuous context flow where each agent has access to what the previous agent produced.</li> <li>Invokes the appropriate agent with the user's query and context from previous interactions</li> <li>Displays the agent's response</li> <li>Continues the loop until the user indicates they're done</li> </ol>"},{"location":"humanInTheLoop_example/#agent-specific-behavior","title":"Agent-Specific Behavior","text":"<ul> <li>ArxivAgent: Converts user query into a search query, retrieves relevant papers, and summarizes results</li> <li>ExecutionAgent: Processes user instructions in the context of previous outputs, can execute code</li> <li>PlanningAgent: Creates plans based on user requirements and previous context</li> <li>WebSearchAgent: Performs web searches based on user queries</li> <li>RecallAgent: Retrieves relevant information from persistent memory</li> <li>Chatter: Provides direct access to the language model for general conversation</li> </ul>"},{"location":"humanInTheLoop_example/#usage","title":"Usage","text":"<p>Run the script and interact with agents by prefixing your queries with the appropriate agent tag:</p> <pre><code>[Arxiver] Find recent papers on transformer architecture improvements\n[Executor] Write a Python function to calculate Fibonacci numbers\n[Planner] Create a research plan for analyzing climate data\n[WebSearcher] What are the latest developments in quantum computing?\n[Rememberer] What did we discuss about neural networks earlier?\n[Chatter] Explain the concept of attention mechanisms in simple terms\n</code></pre>"},{"location":"humanInTheLoop_example/#context-continuity","title":"Context Continuity","text":"<p>A key feature of this interface is that each agent receives both your current query AND the output from the previous agent interaction. This allows for natural follow-up queries and building on previous results. For example:</p> <ol> <li><code>[WebSearcher] Find information about large language models</code></li> <li><code>[Planner] Create a research plan based on this information</code></li> </ol> <p>In this sequence, the Planner would receive both your planning request AND the search results about large language models from the WebSearcher, enabling it to create a more informed and contextually relevant plan.</p> <p>Use \"[USER DONE]\" to exit the interface.</p>"},{"location":"hypothesizer_agent/","title":"HypothesizerAgent Documentation","text":"<p><code>HypothesizerAgent</code> is a multi-agent system that iteratively refines solutions to complex problems through a structured debate process. It employs three specialized agents that work together to generate, critique, and provide alternative perspectives on potential solutions.</p>"},{"location":"hypothesizer_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import HypothesizerAgent\n\n# Initialize the agent\nagent = HypothesizerAgent()\n\n# Run the agent with a question\nsolution = agent.invoke(\n    prompt=\"Find a city with at least 10 vowels in its name.\",\n    max_iter=3\n)\n\n# Print the final solution\nprint(solution)\n</code></pre>"},{"location":"hypothesizer_agent/#parameters","title":"Parameters","text":""},{"location":"hypothesizer_agent/#initialization","title":"Initialization","text":"Parameter Type Default Description <code>llm</code> str or BaseChatModel \"openai/o3-mini\" The language model to use for all agents <code>**kwargs</code> dict {} Additional parameters passed to the base agent"},{"location":"hypothesizer_agent/#run-method","title":"Run Method","text":"Parameter Type Default Description <code>prompt</code> str Required The question or problem to solve <code>max_iter</code> int 3 Maximum number of refinement iterations <code>recursion_limit</code> int 99999 Maximum recursion depth for the graph"},{"location":"hypothesizer_agent/#how-it-works","title":"How It Works","text":"<p>The system uses a three-agent debate process:</p> <ol> <li>Agent 1 (Hypothesizer): Generates initial solutions and refines them based on feedback</li> <li>Agent 2 (Critic): Identifies flaws, assumptions, and areas for improvement</li> <li>Agent 3 (Competitor/Stakeholder): Provides alternative perspectives from different stakeholders</li> </ol> <p>The process iterates through these agents multiple times, with each iteration building on the feedback from previous rounds.</p>"},{"location":"hypothesizer_agent/#features","title":"Features","text":"<ul> <li>Web Search Integration: Uses DuckDuckGo to gather information for each agent</li> <li>Iterative Refinement: Solutions improve through multiple rounds of critique and revision</li> <li>LaTeX Report Generation: Creates a comprehensive LaTeX document summarizing the process</li> <li>URL Tracking: Records all websites visited during the research process</li> </ul>"},{"location":"hypothesizer_agent/#output","title":"Output","text":"<p>The agent produces:</p> <ol> <li>A final refined solution (returned by the <code>run</code> method)</li> <li>A LaTeX document with:</li> <li>Executive summary of the iterative process</li> <li>Final solution in full</li> <li>Detailed appendix of all iterations</li> <li>List of websites visited during research</li> <li>A text file containing the full history of all iterations</li> </ol>"},{"location":"hypothesizer_agent/#example","title":"Example","text":"<pre><code>from ursa.agents import HypothesizerAgent\n\n# Initialize with a specific LLM\nagent = HypothesizerAgent(llm=\"openai/gpt-4o\")\n\n# Run with 5 iterations\nresult = agent.invoke(\n    prompt=\"What strategies could a small local bookstore use to compete with large online retailers?\",\n    max_iter=5\n)\n\nprint(result)\n</code></pre>"},{"location":"hypothesizer_agent/#notes","title":"Notes","text":"<ul> <li>Each iteration includes a solution, critique, and competitor perspective</li> <li>The agent performs web searches to gather information at each step</li> <li>The final LaTeX document provides a comprehensive record of the reasoning process</li> <li>Higher <code>max_iter</code> values produce more refined solutions but take longer to complete</li> </ul>"},{"location":"planning_agent/","title":"PlanningAgent Documentation","text":"<p><code>PlanningAgent</code> is a class that implements a multi-step planning approach for complex problem solving. It uses a state machine architecture to generate plans, reflect on them, and formalize the final solution.</p>"},{"location":"planning_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import PlanningAgent\n\n# Initialize the agent\nagent = PlanningAgent()\n\n# Run a planning task\nresult = agent.invoke(\"Find a city with at least 10 vowels in its name.\")\n\n# Access the final plan\nplan_steps = result[\"plan_steps\"]\n</code></pre>"},{"location":"planning_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>PlanningAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> str or BaseChatModel \"openai/gpt-4o-mini\" The LLM model to use for planning <code>**kwargs</code> dict {} Additional parameters passed to the base agent"},{"location":"planning_agent/#features","title":"Features","text":""},{"location":"planning_agent/#multi-step-planning","title":"Multi-step Planning","text":"<p>The agent follows a three-stage planning process:</p> <ol> <li>Generation: Creates an initial plan to solve the problem</li> <li>Reflection: Critically evaluates and improves the plan</li> <li>Formalization: Structures the final plan as a JSON object</li> </ol>"},{"location":"planning_agent/#structured-output","title":"Structured Output","text":"<p>The final output includes:</p> <ul> <li><code>messages</code>: The conversation history</li> <li><code>plan_steps</code>: A structured list of steps to solve the problem</li> </ul>"},{"location":"planning_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"planning_agent/#customizing-reflection-steps","title":"Customizing Reflection Steps","text":"<p>You can adjust how many reflection iterations the agent performs:</p> <pre><code># Initialize with custom reflection steps\ninitial_state = {\n    \"messages\": [HumanMessage(content=\"Your complex problem here\")],\n    \"reflection_steps\": 5  # Default is 3\n}\n\nresult = agent.invoke(initial_state, {\"configurable\": {\"thread_id\": agent.thread_id}})\n</code></pre>"},{"location":"planning_agent/#streaming-results","title":"Streaming Results","text":"<p>You can stream the agent's thinking process:</p> <pre><code>for event in agent.stream(\n    {\"messages\": [HumanMessage(content=\"Your problem here\")]},\n    {\"configurable\": {\"thread_id\": agent.thread_id}}\n):\n    print(event[list(event.keys())[0]][\"messages\"][-1].content)\n</code></pre>"},{"location":"planning_agent/#setting-a-recursion-limit","title":"Setting a Recursion Limit","text":"<p>For complex planning tasks, you may need to adjust the recursion limit:</p> <pre><code>result = agent.invoke(\n    \"Solve this complex problem...\", \n    recursion_limit=200  # Default is 100\n)\n</code></pre>"},{"location":"planning_agent/#how-it-works","title":"How It Works","text":"<ol> <li>State Machine: The agent uses a directed graph to manage its workflow:</li> <li><code>generate</code> node: Creates or improves the plan</li> <li><code>reflect</code> node: Evaluates the plan for improvements</li> <li> <p><code>formalize</code> node: Structures the final plan as JSON</p> </li> <li> <p>Termination Conditions: The planning process ends when either:</p> </li> <li>The agent has completed the specified number of reflection steps</li> <li> <p>The agent explicitly marks the plan as \"[APPROVED]\"</p> </li> <li> <p>JSON Output: The final plan is structured as a JSON array of steps, each containing:</p> </li> <li>A description of the step</li> <li>Any relevant details for executing that step</li> </ol>"},{"location":"planning_agent/#notes","title":"Notes","text":"<ul> <li>The agent continues to refine its plan through multiple reflection cycles</li> <li>The final output is a structured JSON representation of the solution steps</li> <li>You can access the complete conversation history in the <code>messages</code> field of the result</li> </ul>"},{"location":"web_search_agent/","title":"WebSearchAgent Documentation","text":"<p><code>WebSearchAgent</code> is a powerful tool for conducting internet-based research on any topic. It leverages language models and web search capabilities to gather, process, and summarize information from online sources.</p>"},{"location":"web_search_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import WebSearchAgent\nfrom langchain_openai import ChatOpenAI\n\n# Initialize with default model (gpt-4o-mini)\nwebsearcher = WebSearchAgent()\n\n# Or initialize with a custom model\nmodel = ChatOpenAI(model=\"gpt-4o\", max_tokens=10000)\nwebsearcher = WebSearchAgent(llm=model)\n\n# Run a web search query\nresult = websearcher.invoke(\"Who are the 2025 Detroit Tigers top 10 prospects and what year were they born?\")\n\n# Access the web search results\nfinal_summary = result[\"messages\"][-1].content\nsources = result[\"urls_visited\"]\n\nprint(\"Web Search Summary:\")\nprint(final_summary)\nprint(\"Sources:\", sources)\n</code></pre>"},{"location":"web_search_agent/#parameters","title":"Parameters","text":""},{"location":"web_search_agent/#initialization","title":"Initialization","text":"Parameter Type Default Description <code>llm</code> str or BaseChatModel \"openai/gpt-4o-mini\" The language model to use for web search <code>**kwargs</code> dict {} Additional parameters passed to the base agent"},{"location":"web_search_agent/#run-method","title":"Run Method","text":"Parameter Type Default Description <code>prompt</code> str Required The web search question or topic <code>recursion_limit</code> int 100 Maximum recursion depth for the web search process"},{"location":"web_search_agent/#features","title":"Features","text":"<ul> <li>Automated Web Search: Uses DuckDuckGo to find relevant information</li> <li>Content Processing: Extracts and summarizes content from web pages</li> <li>Iterative Web Search: Continues researching until sufficient information is gathered</li> <li>Source Tracking: Records all URLs visited during research</li> <li>Internet Connectivity Check: Verifies internet access before attempting research</li> </ul>"},{"location":"web_search_agent/#output","title":"Output","text":"<p>The agent returns a dictionary containing:</p> <ul> <li><code>messages</code>: A list of message objects, with the final message containing the comprehensive web search summary</li> <li><code>urls_visited</code>: A list of all sources consulted during the web search process</li> </ul>"},{"location":"web_search_agent/#advanced-usage","title":"Advanced Usage","text":"<pre><code>from ursa.agents import WebSearchAgent\n\n# Initialize with custom parameters\nwebsearcher = WebSearchAgent(\n    llm=\"openai/gpt-4o\",\n    url=\"https://www.example.com\"  # Custom URL for internet connectivity check\n)\n\n# Run with higher recursion limit for complex topics\nresult = websearcher.invoke(\n    \"What are the latest developments in quantum computing? Summarize in markdown format.\",\n    recursion_limit=200\n)\n</code></pre>"},{"location":"web_search_agent/#notes","title":"Notes","text":"<ul> <li>The agent requires internet connectivity to function properly</li> <li>Rate limiting is implemented to avoid overwhelming search services</li> <li>For networks with SSL inspection, you may need to set the <code>CERT_FILE</code> environment variable</li> <li>The websearch process includes multiple steps: search, content processing, review, and final summarization</li> </ul>"},{"location":"api_reference/agents/","title":"agents","text":""},{"location":"api_reference/agents/#ursa.agents.base","title":"<code>base</code>","text":""},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent","title":"<code>BaseAgent</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>class BaseAgent(ABC):\n    # llm: BaseChatModel\n    # llm_with_tools: Runnable[LanguageModelInput, BaseMessage]\n\n    def __init__(\n        self,\n        llm: str | BaseChatModel,\n        checkpointer: BaseCheckpointSaver = None,\n        enable_metrics: bool = False,  # default to enabling metrics\n        metrics_dir: str = \".ursa_metrics\",  # dir to save metrics, with a default\n        autosave_metrics: bool = True,\n        thread_id: Optional[str] = None,\n        **kwargs,\n    ):\n        match llm:\n            case BaseChatModel():\n                self.llm = llm\n\n            case str():\n                self.llm_provider, self.llm_model = llm.split(\"/\")\n                self.llm = ChatLiteLLM(\n                    model=llm,\n                    max_tokens=kwargs.pop(\"max_tokens\", 10000),\n                    max_retries=kwargs.pop(\"max_retries\", 2),\n                    **kwargs,\n                )\n\n            case _:\n                raise TypeError(\n                    \"llm argument must be a string with the provider and model, or a BaseChatModel instance.\"\n                )\n\n        self.thread_id = thread_id or uuid4().hex\n        self.checkpointer = checkpointer\n        self.telemetry = Telemetry(\n            enable=enable_metrics,\n            output_dir=metrics_dir,\n            save_json_default=autosave_metrics,\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Agent name.\"\"\"\n        return self.__class__.__name__\n\n    def add_node(\n        self,\n        graph: StateGraph,\n        f: Callable[..., Mapping[str, Any]],\n        node_name: Optional[str] = None,\n        agent_name: Optional[str] = None,\n    ) -&gt; StateGraph:\n        \"\"\"Add node to graph.\n\n        This is used to track token usage and is simply the following.\n\n        ```python\n        _node_name = node_name or f.__name__\n        return graph.add_node(\n            _node_name, self._wrap_node(f, _node_name, self.name)\n        )\n        ```\n        \"\"\"\n        _node_name = node_name or f.__name__\n        _agent_name = agent_name or _to_snake(self.name)\n        wrapped_node = self._wrap_node(f, _node_name, _agent_name)\n        return graph.add_node(_node_name, wrapped_node)\n\n    def write_state(self, filename, state):\n        json_state = dumps(state, ensure_ascii=False)\n        with open(filename, \"w\") as f:\n            f.write(json_state)\n\n    # BaseAgent\n    def build_config(self, **overrides) -&gt; dict:\n        \"\"\"\n        Build a config dict that includes telemetry callbacks and the thread_id.\n        You can pass overrides like recursion_limit=..., configurable={...}, etc.\n        \"\"\"\n        base = {\n            \"configurable\": {\"thread_id\": self.thread_id},\n            \"metadata\": {\n                \"thread_id\": self.thread_id,\n                \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n            },\n            # \"configurable\": {\n            #     \"thread_id\": getattr(self, \"thread_id\", \"default\")\n            # },\n            # \"metadata\": {\n            #     \"thread_id\": getattr(self, \"thread_id\", \"default\"),\n            #     \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n            # },\n            \"tags\": [self.name],\n            \"callbacks\": self.telemetry.callbacks,\n        }\n        # include model name when we can\n        model_name = getattr(self, \"llm_model\", None) or getattr(\n            getattr(self, \"llm\", None), \"model\", None\n        )\n        if model_name:\n            base[\"metadata\"][\"model\"] = model_name\n\n        if \"configurable\" in overrides and isinstance(\n            overrides[\"configurable\"], dict\n        ):\n            base[\"configurable\"].update(overrides.pop(\"configurable\"))\n        if \"metadata\" in overrides and isinstance(overrides[\"metadata\"], dict):\n            base[\"metadata\"].update(overrides.pop(\"metadata\"))\n        # merge tags if caller provides them\n        if \"tags\" in overrides and isinstance(overrides[\"tags\"], list):\n            base[\"tags\"] = base[\"tags\"] + [\n                t for t in overrides.pop(\"tags\") if t not in base[\"tags\"]\n            ]\n        base.update(overrides)\n        return base\n\n    # agents will invoke like this:\n    # planning_output = planner.invoke(\n    #     {\"messages\": [HumanMessage(content=problem)]},\n    #     config={\n    #         \"recursion_limit\": 999_999,\n    #         \"configurable\": {\"thread_id\": planner.thread_id},\n    #     },\n    # )\n    # they can also, separately, override these defaults about metrics\n    # keys that are NOT inputs; they should not be folded into the inputs mapping\n    _TELEMETRY_KW = {\n        \"raw_debug\",\n        \"save_json\",\n        \"metrics_path\",\n        \"save_raw_snapshot\",\n        \"save_raw_records\",\n    }\n    _CONTROL_KW = {\"config\", \"recursion_limit\", \"tags\", \"metadata\", \"callbacks\"}\n\n    @final\n    def invoke(\n        self,\n        inputs: Optional[InputLike] = None,  # sentinel\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,  # may contain inputs (keyword-inputs) and/or control kw\n    ) -&gt; Any:\n        depth = _INVOKE_DEPTH.get()\n        _INVOKE_DEPTH.set(depth + 1)\n        try:\n            if depth == 0:\n                self.telemetry.begin_run(\n                    agent=self.name, thread_id=self.thread_id\n                )\n\n            # If no positional inputs were provided, split kwargs into inputs vs control\n            if inputs is None:\n                kw_inputs: dict[str, Any] = {}\n                control_kwargs: dict[str, Any] = {}\n                for k, v in kwargs.items():\n                    if k in self._TELEMETRY_KW or k in self._CONTROL_KW:\n                        control_kwargs[k] = v\n                    else:\n                        kw_inputs[k] = v\n                inputs = kw_inputs\n                kwargs = control_kwargs  # only control kwargs remain\n\n            # If both positional inputs and extra unknown kwargs-as-inputs are given, forbid merging\n            else:\n                # keep only control kwargs; anything else would be ambiguous\n                for k in kwargs.keys():\n                    if not (k in self._TELEMETRY_KW or k in self._CONTROL_KW):\n                        raise TypeError(\n                            f\"Unexpected keyword argument '{k}'. \"\n                            \"Pass inputs as a single mapping or omit the positional \"\n                            \"inputs and pass them as keyword arguments.\"\n                        )\n\n            # subclasses may translate keys\n            normalized = self._normalize_inputs(inputs)\n\n            # forward config + any control kwargs (e.g., recursion_limit) to the agent\n            return self._invoke(normalized, config=config, **kwargs)\n\n        finally:\n            new_depth = _INVOKE_DEPTH.get() - 1\n            _INVOKE_DEPTH.set(new_depth)\n            if new_depth == 0:\n                self.telemetry.render(\n                    raw=raw_debug,\n                    save_json=save_json,\n                    filepath=metrics_path,\n                    save_raw_snapshot=save_raw_snapshot,\n                    save_raw_records=save_raw_records,\n                )\n\n    def _normalize_inputs(self, inputs: InputLike) -&gt; Mapping[str, Any]:\n        if isinstance(inputs, str):\n            # Adjust to your message type\n            from langchain_core.messages import HumanMessage\n\n            return {\"messages\": [HumanMessage(content=inputs)]}\n        if isinstance(inputs, Mapping):\n            return inputs\n        raise TypeError(f\"Unsupported input type: {type(inputs)}\")\n\n    @abstractmethod\n    def _invoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n        \"\"\"Subclasses implement the actual work against normalized inputs.\"\"\"\n        ...\n\n    def __call__(self, inputs: InputLike, /, **kwargs: Any) -&gt; Any:\n        return self.invoke(inputs, **kwargs)\n\n    # Runtime enforcement: forbid subclasses from overriding invoke\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if \"invoke\" in cls.__dict__:\n            raise TypeError(\n                f\"{cls.__name__} must not override BaseAgent.invoke(); implement _invoke() only.\"\n            )\n\n    def stream(\n        self,\n        inputs: InputLike,\n        config: Any | None = None,  # allow positional/keyword like LangGraph\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: bool | None = None,\n        metrics_path: str | None = None,\n        save_raw_snapshot: bool | None = None,\n        save_raw_records: bool | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[Any]:\n        \"\"\"Public streaming entry point. Telemetry-wrapped.\"\"\"\n        depth = _INVOKE_DEPTH.get()\n        _INVOKE_DEPTH.set(depth + 1)\n        try:\n            if depth == 0:\n                self.telemetry.begin_run(\n                    agent=self.name, thread_id=self.thread_id\n                )\n            normalized = self._normalize_inputs(inputs)\n            yield from self._stream(normalized, config=config, **kwargs)\n        finally:\n            new_depth = _INVOKE_DEPTH.get() - 1\n            _INVOKE_DEPTH.set(new_depth)\n            if new_depth == 0:\n                self.telemetry.render(\n                    raw=raw_debug,\n                    save_json=save_json,\n                    filepath=metrics_path,\n                    save_raw_snapshot=save_raw_snapshot,\n                    save_raw_records=save_raw_records,\n                )\n\n    def _stream(\n        self,\n        inputs: Mapping[str, Any],\n        *,\n        config: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[Any]:\n        raise NotImplementedError(\n            f\"{self.name} does not support streaming. \"\n            \"Override _stream(...) in your agent to enable it.\"\n        )\n\n    # def run(\n    #     self,\n    #     *args,\n    #     raw_debug: bool = False,\n    #     save_json: bool | None = None,\n    #     metrics_path: str | None = None,\n    #     save_raw_snapshot: bool | None = None,\n    #     save_raw_records: bool | None = None,\n    #     **kwargs\n    # ):\n    #     try:\n    #         self.telemetry.begin_run(agent=self.name, thread_id=self.thread_id)\n    #         result = self._run_impl(*args, **kwargs)\n    #         return result\n    #     finally:\n    #         print(self.telemetry.render(\n    #             raw=raw_debug,\n    #             save_json=save_json,\n    #             filepath=metrics_path,\n    #             save_raw_snapshot=save_raw_snapshot,\n    #             save_raw_records=save_raw_records,\n    #         ))\n\n    # @abstractmethod\n    # def _run_impl(self, *args, **kwargs):\n    #     raise NotImplementedError(\"Agents must implement _run_impl\")\n\n    def _default_node_tags(\n        self, name: str, extra: Sequence[str] | None = None\n    ) -&gt; list[str]:\n        tags = [self.name, \"graph\", name]\n        if extra:\n            tags.extend(extra)\n        return tags\n\n    def _as_runnable(self, fn: Any):\n        # If it's already runnable (has .with_config/.invoke), return it; else wrap\n        return (\n            fn\n            if hasattr(fn, \"with_config\") and hasattr(fn, \"invoke\")\n            else RunnableLambda(fn)\n        )\n\n    def _node_cfg(self, name: str, *extra_tags: str) -&gt; dict:\n        \"\"\"Build a consistent config for a node/runnable so we can reapply it after .map(), subgraph compile, etc.\"\"\"\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n        tags = [self.name, \"graph\", name, *extra_tags]\n        return dict(\n            run_name=\"node\",  # keep \"node:\" prefixing in the timer; don't fight Rich labels here\n            tags=tags,\n            metadata={\n                \"langgraph_node\": name,\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n\n    def ns(self, runnable_or_fn, name: str, *extra_tags: str):\n        \"\"\"Return a runnable with our node config applied. Safe to call on callables or runnables.\n        IMPORTANT: call this AGAIN after .map() / subgraph .compile() (they often drop config).\"\"\"\n        r = self._as_runnable(runnable_or_fn)\n        return r.with_config(**self._node_cfg(name, *extra_tags))\n\n    def _wrap_node(self, fn_or_runnable, name: str, *extra_tags: str):\n        return self.ns(fn_or_runnable, name, *extra_tags)\n\n    def _wrap_cond(self, fn: Any, name: str, *extra_tags: str):\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n        return RunnableLambda(fn).with_config(\n            run_name=\"node\",\n            tags=[\n                self.name,\n                \"graph\",\n                f\"route:{name}\",\n                *extra_tags,\n            ],\n            metadata={\n                \"langgraph_node\": f\"route:{name}\",\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n\n    def _named(self, runnable: Any, name: str, *extra_tags: str):\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n        return runnable.with_config(\n            run_name=name,\n            tags=[self.name, \"graph\", name, *extra_tags],\n            metadata={\n                \"langgraph_node\": name,\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.name","title":"<code>name</code>  <code>property</code>","text":"<p>Agent name.</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.add_node","title":"<code>add_node(graph, f, node_name=None, agent_name=None)</code>","text":"<p>Add node to graph.</p> <p>This is used to track token usage and is simply the following.</p> <pre><code>_node_name = node_name or f.__name__\nreturn graph.add_node(\n    _node_name, self._wrap_node(f, _node_name, self.name)\n)\n</code></pre> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def add_node(\n    self,\n    graph: StateGraph,\n    f: Callable[..., Mapping[str, Any]],\n    node_name: Optional[str] = None,\n    agent_name: Optional[str] = None,\n) -&gt; StateGraph:\n    \"\"\"Add node to graph.\n\n    This is used to track token usage and is simply the following.\n\n    ```python\n    _node_name = node_name or f.__name__\n    return graph.add_node(\n        _node_name, self._wrap_node(f, _node_name, self.name)\n    )\n    ```\n    \"\"\"\n    _node_name = node_name or f.__name__\n    _agent_name = agent_name or _to_snake(self.name)\n    wrapped_node = self._wrap_node(f, _node_name, _agent_name)\n    return graph.add_node(_node_name, wrapped_node)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.build_config","title":"<code>build_config(**overrides)</code>","text":"<p>Build a config dict that includes telemetry callbacks and the thread_id. You can pass overrides like recursion_limit=..., configurable={...}, etc.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def build_config(self, **overrides) -&gt; dict:\n    \"\"\"\n    Build a config dict that includes telemetry callbacks and the thread_id.\n    You can pass overrides like recursion_limit=..., configurable={...}, etc.\n    \"\"\"\n    base = {\n        \"configurable\": {\"thread_id\": self.thread_id},\n        \"metadata\": {\n            \"thread_id\": self.thread_id,\n            \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n        },\n        # \"configurable\": {\n        #     \"thread_id\": getattr(self, \"thread_id\", \"default\")\n        # },\n        # \"metadata\": {\n        #     \"thread_id\": getattr(self, \"thread_id\", \"default\"),\n        #     \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n        # },\n        \"tags\": [self.name],\n        \"callbacks\": self.telemetry.callbacks,\n    }\n    # include model name when we can\n    model_name = getattr(self, \"llm_model\", None) or getattr(\n        getattr(self, \"llm\", None), \"model\", None\n    )\n    if model_name:\n        base[\"metadata\"][\"model\"] = model_name\n\n    if \"configurable\" in overrides and isinstance(\n        overrides[\"configurable\"], dict\n    ):\n        base[\"configurable\"].update(overrides.pop(\"configurable\"))\n    if \"metadata\" in overrides and isinstance(overrides[\"metadata\"], dict):\n        base[\"metadata\"].update(overrides.pop(\"metadata\"))\n    # merge tags if caller provides them\n    if \"tags\" in overrides and isinstance(overrides[\"tags\"], list):\n        base[\"tags\"] = base[\"tags\"] + [\n            t for t in overrides.pop(\"tags\") if t not in base[\"tags\"]\n        ]\n    base.update(overrides)\n    return base\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.ns","title":"<code>ns(runnable_or_fn, name, *extra_tags)</code>","text":"<p>Return a runnable with our node config applied. Safe to call on callables or runnables. IMPORTANT: call this AGAIN after .map() / subgraph .compile() (they often drop config).</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def ns(self, runnable_or_fn, name: str, *extra_tags: str):\n    \"\"\"Return a runnable with our node config applied. Safe to call on callables or runnables.\n    IMPORTANT: call this AGAIN after .map() / subgraph .compile() (they often drop config).\"\"\"\n    r = self._as_runnable(runnable_or_fn)\n    return r.with_config(**self._node_cfg(name, *extra_tags))\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.stream","title":"<code>stream(inputs, config=None, /, *, raw_debug=False, save_json=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, **kwargs)</code>","text":"<p>Public streaming entry point. Telemetry-wrapped.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def stream(\n    self,\n    inputs: InputLike,\n    config: Any | None = None,  # allow positional/keyword like LangGraph\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: bool | None = None,\n    metrics_path: str | None = None,\n    save_raw_snapshot: bool | None = None,\n    save_raw_records: bool | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[Any]:\n    \"\"\"Public streaming entry point. Telemetry-wrapped.\"\"\"\n    depth = _INVOKE_DEPTH.get()\n    _INVOKE_DEPTH.set(depth + 1)\n    try:\n        if depth == 0:\n            self.telemetry.begin_run(\n                agent=self.name, thread_id=self.thread_id\n            )\n        normalized = self._normalize_inputs(inputs)\n        yield from self._stream(normalized, config=config, **kwargs)\n    finally:\n        new_depth = _INVOKE_DEPTH.get() - 1\n        _INVOKE_DEPTH.set(new_depth)\n        if new_depth == 0:\n            self.telemetry.render(\n                raw=raw_debug,\n                save_json=save_json,\n                filepath=metrics_path,\n                save_raw_snapshot=save_raw_snapshot,\n                save_raw_records=save_raw_records,\n            )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent","title":"<code>code_review_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.code_review_agent.read_file","title":"<code>read_file(filename, state)</code>","text":"<p>Reads in a file with a given filename into a string</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>string filename to read in</p> required Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef read_file(filename: str, state: Annotated[dict, InjectedState]):\n    \"\"\"\n    Reads in a file with a given filename into a string\n\n    Args:\n        filename: string filename to read in\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    full_filename = os.path.join(workspace_dir, filename)\n\n    print(\"[READING]: \", full_filename)\n    with open(full_filename, \"r\") as file:\n        file_contents = file.read()\n    return file_contents\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run command from commandline</p> Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"Run command from commandline\"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"RUNNING: \", query)\n    process = subprocess.Popen(\n        query.split(\" \"),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        cwd=workspace_dir,\n    )\n\n    stdout, stderr = process.communicate(timeout=600)\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent.write_file","title":"<code>write_file(code, filename, state)</code>","text":"<p>Writes text to a file in the given workspace as requested.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Text to write to a file</p> required <code>filename</code> <code>str</code> <p>the filename to write to</p> required <p>Returns:</p> Type Description <code>str</code> <p>Execution results</p> Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef write_file(\n    code: str, filename: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Writes text to a file in the given workspace as requested.\n\n    Args:\n        code: Text to write to a file\n        filename: the filename to write to\n\n    Returns:\n        Execution results\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"[WRITING]: \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent","title":"<code>execution_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent","title":"<code>ExecutionAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>class ExecutionAgent(BaseAgent):\n    def __init__(\n        self,\n        llm: str | BaseChatModel = \"openai/gpt-4o-mini\",\n        agent_memory: Optional[Any | AgentMemory] = None,\n        log_state: bool = False,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.agent_memory = agent_memory\n        self.safety_prompt = safety_prompt\n        self.executor_prompt = executor_prompt\n        self.summarize_prompt = summarize_prompt\n        self.tools = [run_cmd, write_code, edit_code, search_tool]\n        self.tool_node = ToolNode(self.tools)\n        self.llm = self.llm.bind_tools(self.tools)\n        self.log_state = log_state\n\n        self._action = self._build_graph()\n\n    # Define the function that calls the model\n    def query_executor(self, state: ExecutionState) -&gt; ExecutionState:\n        new_state = state.copy()\n        if \"workspace\" not in new_state.keys():\n            new_state[\"workspace\"] = randomname.get_name()\n            print(\n                f\"{RED}Creating the folder {BLUE}{BOLD}{new_state['workspace']}{RESET}{RED} for this project.{RESET}\"\n            )\n        os.makedirs(new_state[\"workspace\"], exist_ok=True)\n\n        # code related to symlink\n        sd = new_state.get(\"symlinkdir\")\n        if isinstance(sd, dict) and \"is_linked\" not in sd:\n            # symlinkdir = {\"source\": \"foo\", \"dest\": \"bar\"}\n            symlinkdir = new_state[\"symlinkdir\"]\n            # user provided a symlinkdir key - let's do the linking!\n\n            src = Path(symlinkdir[\"source\"]).expanduser().resolve()\n            workspace_root = Path(new_state[\"workspace\"]).expanduser().resolve()\n            dst = workspace_root / symlinkdir[\"dest\"]  # prepend workspace\n\n            # if you want to replace an existing link/file, unlink it first\n            if dst.exists() or dst.is_symlink():\n                dst.unlink()\n\n            # create parent dirs for the link location if they don\u2019t exist\n            dst.parent.mkdir(parents=True, exist_ok=True)\n\n            # actually make the link (tell pathlib it\u2019s a directory target)\n            dst.symlink_to(src, target_is_directory=src.is_dir())\n            print(f\"{RED}Symlinked {src} (source) --&gt; {dst} (dest)\")\n            # note that we've done the symlink now, so don't need to do it later\n            new_state[\"symlinkdir\"][\"is_linked\"] = True\n\n        if isinstance(new_state[\"messages\"][0], SystemMessage):\n            new_state[\"messages\"][0] = SystemMessage(\n                content=self.executor_prompt\n            )\n        else:\n            new_state[\"messages\"] = [\n                SystemMessage(content=self.executor_prompt)\n            ] + state[\"messages\"]\n        try:\n            response = self.llm.invoke(\n                new_state[\"messages\"], self.build_config(tags=[\"agent\"])\n            )\n        except ContentPolicyViolationError as e:\n            print(\"Error: \", e, \" \", new_state[\"messages\"][-1].content)\n        if self.log_state:\n            self.write_state(\"execution_agent.json\", new_state)\n        return {\"messages\": [response], \"workspace\": new_state[\"workspace\"]}\n\n    # Define the function that calls the model\n    def summarize(self, state: ExecutionState) -&gt; ExecutionState:\n        messages = [SystemMessage(content=summarize_prompt)] + state[\"messages\"]\n        try:\n            response = self.llm.invoke(\n                messages, self.build_config(tags=[\"summarize\"])\n            )\n        except ContentPolicyViolationError as e:\n            print(\"Error: \", e, \" \", messages[-1].content)\n        if self.agent_memory:\n            memories = []\n            # Handle looping through the messages\n            for x in state[\"messages\"]:\n                if not isinstance(x, AIMessage):\n                    memories.append(x.content)\n                elif not x.tool_calls:\n                    memories.append(x.content)\n                else:\n                    tool_strings = []\n                    for tool in x.tool_calls:\n                        tool_name = \"Tool Name: \" + tool[\"name\"]\n                        tool_strings.append(tool_name)\n                        for y in tool[\"args\"]:\n                            tool_strings.append(\n                                f\"Arg: {str(y)}\\nValue: {str(tool['args'][y])}\"\n                            )\n                    memories.append(\"\\n\".join(tool_strings))\n            memories.append(response.content)\n            self.agent_memory.add_memories(memories)\n            save_state = state.copy()\n            save_state[\"messages\"].append(response)\n        if self.log_state:\n            self.write_state(\"execution_agent.json\", save_state)\n        return {\"messages\": [response.content]}\n\n    # Define the function that calls the model\n    def safety_check(self, state: ExecutionState) -&gt; ExecutionState:\n        \"\"\"\n        Validate the safety of a pending shell command.\n\n        Args:\n            state: Current execution state.\n\n        Returns:\n            Either the unchanged state (safe) or a state with tool message(s) (unsafe).\n        \"\"\"\n        new_state = state.copy()\n        last_msg = new_state[\"messages\"][-1]\n\n        tool_responses = []\n        tool_failed = False\n        for tool_call in last_msg.tool_calls:\n            call_name = tool_call[\"name\"]\n\n            if call_name == \"run_cmd\":\n                query = tool_call[\"args\"][\"query\"]\n                safety_check = self.llm.invoke(\n                    self.safety_prompt + query,\n                    self.build_config(tags=[\"safety_check\"]),\n                )\n\n                if \"[NO]\" in safety_check.content:\n                    tool_failed = True\n\n                    tool_response = f\"\"\"\n                    [UNSAFE] That command `{query}` was deemed unsafe and cannot be run.\n                    For reason: {safety_check.content}\n                    \"\"\"\n                    console.print(\n                        \"[bold red][WARNING][/bold red] Command deemed unsafe:\",\n                        query,\n                    )\n                    # and tell the user the reason\n                    console.print(\n                        \"[bold red][WARNING][/bold red] REASON:\", tool_response\n                    )\n\n                else:\n                    tool_response = f\"Command `{query}` passed safety check.\"\n                    console.print(\n                        f\"[green]Command passed safety check:[/green] {query}\"\n                    )\n\n                tool_responses.append(\n                    ToolMessage(\n                        content=tool_response,\n                        tool_call_id=tool_call[\"id\"],\n                    )\n                )\n\n        if tool_failed:\n            new_state[\"messages\"].extend(tool_responses)\n\n        return new_state\n\n    def _build_graph(self):\n        graph = StateGraph(ExecutionState)\n\n        self.add_node(graph, self.query_executor, \"agent\")\n        self.add_node(graph, self.tool_node, \"action\")\n        self.add_node(graph, self.summarize, \"summarize\")\n        self.add_node(graph, self.safety_check, \"safety_check\")\n\n        # Set the entrypoint as `agent`\n        # This means that this node is the first one called\n        graph.set_entry_point(\"agent\")\n\n        graph.add_conditional_edges(\n            \"agent\",\n            self._wrap_cond(should_continue, \"should_continue\", \"execution\"),\n            {\"continue\": \"safety_check\", \"summarize\": \"summarize\"},\n        )\n\n        graph.add_conditional_edges(\n            \"safety_check\",\n            self._wrap_cond(command_safe, \"command_safe\", \"execution\"),\n            {\"safe\": \"action\", \"unsafe\": \"agent\"},\n        )\n\n        graph.add_edge(\"action\", \"agent\")\n        graph.set_finish_point(\"summarize\")\n\n        return graph.compile(checkpointer=self.checkpointer)\n        # self.action.get_graph().draw_mermaid_png(output_file_path=\"execution_agent_graph.png\", draw_method=MermaidDrawMethod.PYPPETEER)\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 999_999, **_\n    ):\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n        return self._action.invoke(inputs, config)\n\n    # this is trying to stop people bypassing invoke\n    @property\n    def action(self):\n        raise AttributeError(\n            \"Use .stream(...) or .invoke(...); direct .action access is unsupported.\"\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.safety_check","title":"<code>safety_check(state)</code>","text":"<p>Validate the safety of a pending shell command.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>Current execution state.</p> required <p>Returns:</p> Type Description <code>ExecutionState</code> <p>Either the unchanged state (safe) or a state with tool message(s) (unsafe).</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def safety_check(self, state: ExecutionState) -&gt; ExecutionState:\n    \"\"\"\n    Validate the safety of a pending shell command.\n\n    Args:\n        state: Current execution state.\n\n    Returns:\n        Either the unchanged state (safe) or a state with tool message(s) (unsafe).\n    \"\"\"\n    new_state = state.copy()\n    last_msg = new_state[\"messages\"][-1]\n\n    tool_responses = []\n    tool_failed = False\n    for tool_call in last_msg.tool_calls:\n        call_name = tool_call[\"name\"]\n\n        if call_name == \"run_cmd\":\n            query = tool_call[\"args\"][\"query\"]\n            safety_check = self.llm.invoke(\n                self.safety_prompt + query,\n                self.build_config(tags=[\"safety_check\"]),\n            )\n\n            if \"[NO]\" in safety_check.content:\n                tool_failed = True\n\n                tool_response = f\"\"\"\n                [UNSAFE] That command `{query}` was deemed unsafe and cannot be run.\n                For reason: {safety_check.content}\n                \"\"\"\n                console.print(\n                    \"[bold red][WARNING][/bold red] Command deemed unsafe:\",\n                    query,\n                )\n                # and tell the user the reason\n                console.print(\n                    \"[bold red][WARNING][/bold red] REASON:\", tool_response\n                )\n\n            else:\n                tool_response = f\"Command `{query}` passed safety check.\"\n                console.print(\n                    f\"[green]Command passed safety check:[/green] {query}\"\n                )\n\n            tool_responses.append(\n                ToolMessage(\n                    content=tool_response,\n                    tool_call_id=tool_call[\"id\"],\n                )\n            )\n\n    if tool_failed:\n        new_state[\"messages\"].extend(tool_responses)\n\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.command_safe","title":"<code>command_safe(state)</code>","text":"<p>Return graph edge \"safe\" if the last command was safe, otherwise return edge \"unsafe\"</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def command_safe(state: ExecutionState) -&gt; Literal[\"safe\", \"unsafe\"]:\n    \"\"\"\n    Return graph edge \"safe\" if the last command was safe, otherwise return edge \"unsafe\"\n    \"\"\"\n\n    index = -1\n    message = state[\"messages\"][index]\n    # Loop through all the consecutive tool messages in reverse order\n    while isinstance(message, ToolMessage):\n        if \"[UNSAFE]\" in message.content:\n            return \"unsafe\"\n\n        index -= 1\n        message = state[\"messages\"][index]\n\n    return \"safe\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.edit_code","title":"<code>edit_code(old_code, new_code, filename, state)</code>","text":"<p>Replace the first occurrence of old_code with new_code in filename.</p> <p>Parameters:</p> Name Type Description Default <code>old_code</code> <code>str</code> <p>Code fragment to search for.</p> required <code>new_code</code> <code>str</code> <p>Replacement fragment.</p> required <code>filename</code> <code>str</code> <p>Target file inside the workspace.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Success / failure message.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>@tool\ndef edit_code(\n    old_code: str,\n    new_code: str,\n    filename: str,\n    state: Annotated[dict, InjectedState],\n) -&gt; str:\n    \"\"\"Replace the **first** occurrence of *old_code* with *new_code* in *filename*.\n\n    Args:\n        old_code: Code fragment to search for.\n        new_code: Replacement fragment.\n        filename: Target file inside the workspace.\n\n    Returns:\n        Success / failure message.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    console.print(\"[cyan]Editing file:[/cyan]\", filename)\n\n    code_file = os.path.join(workspace_dir, filename)\n    try:\n        with open(code_file, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n    except FileNotFoundError:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]File not found:[/]\",\n            filename,\n        )\n        return f\"Failed: {filename} not found.\"\n\n    # Clean up markdown fences\n    old_code_clean = _strip_fences(old_code)\n    new_code_clean = _strip_fences(new_code)\n\n    if old_code_clean not in content:\n        console.print(\n            \"[yellow] \u26a0\ufe0f 'old_code' not found in file'; no changes made.[/]\"\n        )\n        return f\"No changes made to {filename}: 'old_code' not found in file.\"\n\n    updated = content.replace(old_code_clean, new_code_clean, 1)\n\n    console.print(\n        Panel(\n            DiffRenderer(content, updated, filename),\n            title=\"Diff Preview\",\n            border_style=\"cyan\",\n        )\n    )\n\n    try:\n        with open(code_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(updated)\n    except Exception as exc:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]Failed to write file:[/]\",\n            exc,\n        )\n        return f\"Failed to edit {filename}.\"\n\n    console.print(\n        f\"[bold bright_white on green] :heavy_check_mark: [/] \"\n        f\"[green]File updated:[/] {code_file}\"\n    )\n    return f\"File {filename} updated successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run a commandline command from using the subprocess package in python</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>commandline command to be run as a string given to the subprocess.run command.</p> required Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"\n    Run a commandline command from using the subprocess package in python\n\n    Args:\n        query: commandline command to be run as a string given to the subprocess.run command.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"RUNNING: \", query)\n    try:\n        result = subprocess.run(\n            query,\n            text=True,\n            shell=True,\n            timeout=60000,\n            capture_output=True,\n            cwd=workspace_dir,\n        )\n        stdout, stderr = result.stdout, result.stderr\n    except KeyboardInterrupt:\n        print(\"Keyboard Interrupt of command: \", query)\n        stdout, stderr = \"\", \"KeyboardInterrupt:\"\n\n    # Fit BOTH streams under a single overall cap\n    stdout_fit, stderr_fit = _fit_streams_to_budget(\n        stdout or \"\", stderr or \"\", MAX_TOOL_MSG_CHARS\n    )\n\n    print(\"STDOUT: \", stdout_fit)\n    print(\"STDERR: \", stderr_fit)\n\n    return f\"STDOUT:\\n{stdout_fit}\\nSTDERR:\\n{stderr_fit}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.write_code","title":"<code>write_code(code, filename, tool_call_id, state)</code>","text":"<p>Write code to filename.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Source code as a string.</p> required <code>filename</code> <code>str</code> <p>Target filename (including extension).</p> required <p>Returns:</p> Type Description <code>Command</code> <p>Success / failure message.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>@tool\ndef write_code(\n    code: str,\n    filename: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    state: Annotated[dict, InjectedState],\n) -&gt; Command:\n    \"\"\"Write *code* to *filename*.\n\n    Args:\n        code: Source code as a string.\n        filename: Target filename (including extension).\n\n    Returns:\n        Success / failure message.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    console.print(\"[cyan]Writing file:[/]\", filename)\n\n    # Clean up markdown fences\n    code = _strip_fences(code)\n\n    # Syntax-highlighted preview\n    try:\n        lexer_name = Syntax.guess_lexer(filename, code)\n    except Exception:\n        lexer_name = \"text\"\n\n    console.print(\n        Panel(\n            Syntax(code, lexer_name, line_numbers=True),\n            title=\"File Preview\",\n            border_style=\"cyan\",\n        )\n    )\n\n    code_file = os.path.join(workspace_dir, filename)\n    try:\n        with open(code_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(code)\n    except Exception as exc:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]Failed to write file:[/]\",\n            exc,\n        )\n        return f\"Failed to write {filename}.\"\n\n    console.print(\n        f\"[bold bright_white on green] :heavy_check_mark: [/] \"\n        f\"[green]File written:[/] {code_file}\"\n    )\n\n    # Append the file to the list in state\n    file_list = state.get(\"code_files\", [])\n    file_list.append(filename)\n\n    # Create a tool message to send back\n    msg = ToolMessage(\n        content=f\"File {filename} written successfully.\",\n        tool_call_id=tool_call_id,\n    )\n\n    # Return updated code files list &amp; the message\n    return Command(\n        update={\n            \"code_files\": file_list,\n            \"messages\": [msg],\n        }\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent","title":"<code>hypothesizer_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent","title":"<code>HypothesizerAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>class HypothesizerAgent(BaseAgent):\n    def __init__(self, llm: str | BaseChatModel = \"openai/o3-mini\", **kwargs):\n        super().__init__(llm, **kwargs)\n        self.hypothesizer_prompt = hypothesizer_prompt\n        self.critic_prompt = critic_prompt\n        self.competitor_prompt = competitor_prompt\n        self.search_tool = DuckDuckGoSearchResults(\n            output_format=\"json\", num_results=10\n        )\n        # self.search_tool = TavilySearchResults(\n        #     max_results=10, search_depth=\"advanced\", include_answer=False\n        # )\n\n        self._action = self._build_graph()\n\n    def agent1_generate_solution(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"Agent 1: Hypothesizer.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Entering agent1_generate_solution. Iteration: {state['current_iteration']}\"\n        )\n\n        current_iter = state[\"current_iteration\"]\n        user_content = f\"Question: {state['question']}\\n\"\n\n        if current_iter &gt; 0:\n            user_content += (\n                f\"\\nPrevious solution: {state['agent1_solution'][-1]}\"\n            )\n            user_content += f\"\\nCritique: {state['agent2_critiques'][-1]}\"\n            user_content += (\n                f\"\\nCompetitor perspective: {state['agent3_perspectives'][-1]}\"\n            )\n            user_content += (\n                \"\\n\\n**You must explicitly list how this new solution differs from the previous solution,** \"\n                \"point by point, explaining what changes were made in response to the critique and competitor perspective.\"\n                \"\\nAfterward, provide your updated solution.\"\n            )\n        else:\n            user_content += \"Research this problem and generate a solution.\"\n\n        search_query = self.llm.invoke(\n            f\"Here is a problem description: {state['question']}. Turn it into a short query to be fed into a search engine.\"\n        ).content\n        if '\"' in search_query:\n            search_query = search_query.split('\"')[1]\n        raw_search_results = self.search_tool.invoke(search_query)\n\n        # Parse the results if possible, so we can collect URLs\n        new_state = state.copy()\n        new_state[\"question_search_query\"] = search_query\n        if \"visited_sites\" not in new_state:\n            new_state[\"visited_sites\"] = []\n\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    print(f\"[DEBUG] Appending visited link: {link}\")\n                    new_state[\"visited_sites\"].append(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        user_content += f\"\\nSearch results: {raw_search_results}\"\n\n        # Provide a system message to define this agent's role\n        messages = [\n            SystemMessage(content=self.hypothesizer_prompt),\n            HumanMessage(content=user_content),\n        ]\n        solution = self.llm.invoke(messages)\n\n        new_state[\"agent1_solution\"].append(solution.content)\n\n        # Print the entire solution in green\n        print(\n            f\"{GREEN}[Agent1 - Hypothesizer solution]\\n{solution.content}{RESET}\"\n        )\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Exiting agent1_generate_solution.\"\n        )\n        return new_state\n\n    def agent2_critique(self, state: HypothesizerState) -&gt; HypothesizerState:\n        \"\"\"Agent 2: Critic.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Entering agent2_critique.\"\n        )\n\n        solution = state[\"agent1_solution\"][-1]\n        user_content = (\n            f\"Question: {state['question']}\\n\"\n            f\"Proposed solution: {solution}\\n\"\n            \"Provide a detailed critique of this solution. Identify potential flaws, assumptions, and areas for improvement.\"\n        )\n\n        fact_check_query = f\"fact check {state['question_search_query']} solution effectiveness\"\n\n        raw_search_results = self.search_tool.invoke(fact_check_query)\n\n        # Parse the results if possible, so we can collect URLs\n        new_state = state.copy()\n        if \"visited_sites\" not in new_state:\n            new_state[\"visited_sites\"] = []\n\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    print(f\"[DEBUG] Appending visited link: {link}\")\n                    new_state[\"visited_sites\"].append(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        fact_check_results = raw_search_results\n        user_content += f\"\\nFact check results: {fact_check_results}\"\n\n        messages = [\n            SystemMessage(content=self.critic_prompt),\n            HumanMessage(content=user_content),\n        ]\n        critique = self.llm.invoke(messages)\n\n        new_state[\"agent2_critiques\"].append(critique.content)\n\n        # Print the entire critique in blue\n        print(f\"{BLUE}[Agent2 - Critic]\\n{critique.content}{RESET}\")\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Exiting agent2_critique.\"\n        )\n        return new_state\n\n    def agent3_competitor_perspective(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"Agent 3: Competitor/Stakeholder Simulator.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Entering agent3_competitor_perspective.\"\n        )\n\n        solution = state[\"agent1_solution\"][-1]\n        critique = state[\"agent2_critiques\"][-1]\n\n        user_content = (\n            f\"Question: {state['question']}\\n\"\n            f\"Proposed solution: {solution}\\n\"\n            f\"Critique: {critique}\\n\"\n            \"Simulate how a competitor, government agency, or other stakeholder might respond to this solution.\"\n        )\n\n        competitor_search_query = (\n            f\"competitor responses to {state['question_search_query']}\"\n        )\n\n        raw_search_results = self.search_tool.invoke(competitor_search_query)\n\n        # Parse the results if possible, so we can collect URLs\n        new_state = state.copy()\n        if \"visited_sites\" not in new_state:\n            new_state[\"visited_sites\"] = []\n\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    print(f\"[DEBUG] Appending visited link: {link}\")\n                    new_state[\"visited_sites\"].append(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        competitor_info = raw_search_results\n        user_content += f\"\\nCompetitor information: {competitor_info}\"\n\n        messages = [\n            SystemMessage(content=self.competitor_prompt),\n            HumanMessage(content=user_content),\n        ]\n        perspective = self.llm.invoke(messages)\n\n        new_state[\"agent3_perspectives\"].append(perspective.content)\n\n        # Print the entire perspective in red\n        print(\n            f\"{RED}[Agent3 - Competitor/Stakeholder Perspective]\\n{perspective.content}{RESET}\"\n        )\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Exiting agent3_competitor_perspective.\"\n        )\n        return new_state\n\n    def increment_iteration(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        new_state = state.copy()\n        new_state[\"current_iteration\"] += 1\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Iteration incremented to {new_state['current_iteration']}\"\n        )\n        return new_state\n\n    def generate_solution(self, state: HypothesizerState) -&gt; HypothesizerState:\n        \"\"\"Generate the overall, refined solution based on all iterations.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Entering generate_solution.\"\n        )\n        prompt = f\"Original question: {state['question']}\\n\\n\"\n        prompt += \"Evolution of solutions:\\n\"\n\n        for i in range(state[\"max_iterations\"]):\n            prompt += f\"\\nIteration {i + 1}:\\n\"\n            prompt += f\"Solution: {state['agent1_solution'][i]}\\n\"\n            prompt += f\"Critique: {state['agent2_critiques'][i]}\\n\"\n            prompt += (\n                f\"Competitor perspective: {state['agent3_perspectives'][i]}\\n\"\n            )\n\n        prompt += \"\\nBased on this iterative process, provide the overall, refined solution.\"\n\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Generating overall solution with LLM...\"\n        )\n        solution = self.llm.invoke(prompt)\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Overall solution obtained. Preview:\",\n            solution.content[:200],\n            \"...\",\n        )\n\n        new_state = state.copy()\n        new_state[\"solution\"] = solution.content\n\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Exiting generate_solution.\"\n        )\n        return new_state\n\n    def print_visited_sites(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        new_state = state.copy()\n        all_sites = new_state.get(\"visited_sites\", [])\n        print(\"[DEBUG] Visited Sites:\")\n        for s in all_sites:\n            print(\"  \", s)\n        return new_state\n\n    def summarize_process_as_latex(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"\n        Summarize how the solution changed over time, referencing\n        each iteration's critique and competitor perspective,\n        then produce a final LaTeX document.\n        \"\"\"\n        print(\"[DEBUG] Entering summarize_process_as_latex.\")\n        llm_model = state.get(\"llm_model\", \"unknown_model\")\n\n        # Build a single string describing the entire iterative process\n        iteration_details = \"\"\n        for i, (sol, crit, comp) in enumerate(\n            zip(\n                state[\"agent1_solution\"],\n                state[\"agent2_critiques\"],\n                state[\"agent3_perspectives\"],\n            ),\n            start=1,\n        ):\n            iteration_details += (\n                f\"\\\\subsection*{{Iteration {i}}}\\n\\n\"\n                f\"\\\\textbf{{Solution:}}\\\\\\\\\\n{sol}\\n\\n\"\n                f\"\\\\textbf{{Critique:}}\\\\\\\\\\n{crit}\\n\\n\"\n                f\"\\\\textbf{{Competitor Perspective:}}\\\\\\\\\\n{comp}\\n\\n\"\n            )\n\n        # -----------------------------\n        # Write iteration_details to disk as .txt\n        # -----------------------------\n        timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        txt_filename = (\n            f\"iteration_details_{llm_model}_{timestamp_str}_chat_history.txt\"\n        )\n        with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(iteration_details)\n\n        print(f\"[DEBUG] Wrote iteration details to {txt_filename}.\")\n\n        # Prompt the LLM to produce a LaTeX doc\n        # We'll just pass it as a single string to the LLM;\n        # you could also do system+human messages if you prefer.\n        prompt = f\"\"\"\\\n            You are a system that produces a FULL LaTeX document.\n            Here is information about a multi-iteration process:\n\n            Original question: {state[\"question\"]}\n\n            Below are the solutions, critiques, and competitor perspectives from each iteration:\n\n            {iteration_details}\n\n            The solution we arrived at was:\n\n            {state[\"solution\"]}\n\n            Now produce a valid LaTeX document.  Be sure to use a table of contents.\n            It must start with an Executive Summary (that may be multiple pages) which summarizes\n            the entire iterative process.  Following that, we should include the solution in full,\n            not summarized, but reformatted for appropriate LaTeX.  And then, finally (and this will be\n            quite long), we must take all the steps - solutions, critiques, and competitor perspectives\n            and *NOT SUMMARIZE THEM* but merely reformat them for the reader.  This will be in an Appendix\n            of the full content of the steps.  Finally, include a listing of all of the websites we\n            used in our research.\n\n            You must ONLY RETURN LaTeX, nothing else.  It must be valid LaTeX syntax!\n\n            Your output should start with:\n            \\\\documentclass{{article}}\n            \\\\usepackage[margin=1in]{{geometry}}\n            etc.\n\n            It must compile without errors under pdflatex. \n        \"\"\"\n\n        # Now produce a valid LaTeX document that nicely summarizes this entire iterative process.\n        # It must include the overall solution in full, not summarized, but reformatted for appropriate\n        # LaTeX. The summarization is for the other steps.\n\n        all_visited_sites = state.get(\"visited_sites\", [])\n        # (Optional) remove duplicates by converting to a set, then back to a list\n        visited_sites_unique = list(set(all_visited_sites))\n        if visited_sites_unique:\n            websites_latex = \"\\\\section*{Websites Visited}\\\\begin{itemize}\\n\"\n            for url in visited_sites_unique:\n                print(f\"We visited: {url}\")\n                # Use \\url{} to handle special characters in URLs\n                websites_latex += f\"\\\\item \\\\url{{{url}}}\\n\"\n            websites_latex += \"\\\\end{itemize}\\n\\n\"\n        else:\n            # If no sites visited, or the list is empty\n            websites_latex = (\n                \"\\\\section*{Websites Visited}\\nNo sites were visited.\\n\\n\"\n            )\n        print(websites_latex)\n\n        # Ask the LLM to produce *only* LaTeX content\n        latex_response = self.llm.invoke(prompt)\n\n        latex_doc = latex_response.content\n\n        def inject_into_latex(original_tex: str, injection: str) -&gt; str:\n            \"\"\"\n            Find the last occurrence of '\\\\end{document}' in 'original_tex'\n            and insert 'injection' right before it.\n            If '\\\\end{document}' is not found, just append the injection at the end.\n            \"\"\"\n            injection_index = original_tex.rfind(r\"\\end{document}\")\n            if injection_index == -1:\n                # If the LLM didn't include \\end{document}, just append\n                return original_tex + \"\\n\" + injection\n            else:\n                # Insert right before \\end{document}\n                return (\n                    original_tex[:injection_index]\n                    + \"\\n\"\n                    + injection\n                    + \"\\n\"\n                    + original_tex[injection_index:]\n                )\n\n        final_latex = inject_into_latex(latex_doc, websites_latex)\n\n        new_state = state.copy()\n        new_state[\"summary_report\"] = final_latex\n\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Received LaTeX from LLM. Preview:\"\n        )\n        print(latex_response.content[:300], \"...\")\n        print(\n            f\"[iteration {state['current_iteration']} - DEBUG] Exiting summarize_process_as_latex.\"\n        )\n        return new_state\n\n    def _build_graph(self):\n        # Initialize the graph\n        graph = StateGraph(HypothesizerState)\n\n        # Add nodes\n        self.add_node(graph, self.agent1_generate_solution, \"agent1\")\n        self.add_node(graph, self.agent2_critique, \"agent2\")\n        self.add_node(graph, self.agent3_competitor_perspective, \"agent3\")\n        self.add_node(graph, self.increment_iteration, \"increment_iteration\")\n        self.add_node(graph, self.generate_solution, \"finalize\")\n        self.add_node(graph, self.print_visited_sites, \"print_sites\")\n        self.add_node(\n            graph, self.summarize_process_as_latex, \"summarize_as_latex\"\n        )\n        # self.graph.add_node(\"compile_pdf\",                compile_summary_to_pdf)\n\n        # Add simple edges for the known flow\n        graph.add_edge(\"agent1\", \"agent2\")\n        graph.add_edge(\"agent2\", \"agent3\")\n        graph.add_edge(\"agent3\", \"increment_iteration\")\n\n        # Then from increment_iteration, we have a conditional:\n        # If we 'continue', we go back to agent1\n        # If we 'finish', we jump to the finalize node\n        graph.add_conditional_edges(\n            \"increment_iteration\",\n            should_continue,\n            {\"continue\": \"agent1\", \"finish\": \"finalize\"},\n        )\n\n        graph.add_edge(\"finalize\", \"summarize_as_latex\")\n        graph.add_edge(\"summarize_as_latex\", \"print_sites\")\n        # self.graph.add_edge(\"summarize_as_latex\", \"compile_pdf\")\n        # self.graph.add_edge(\"compile_pdf\", \"print_sites\")\n\n        # Set the entry point\n        graph.set_entry_point(\"agent1\")\n        graph.set_finish_point(\"print_sites\")\n\n        return graph.compile(checkpointer=self.checkpointer)\n        # self.action.get_graph().draw_mermaid_png(output_file_path=\"hypothesizer_agent_graph.png\", draw_method=MermaidDrawMethod.PYPPETEER)\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 100000, **_\n    ):\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n        if \"prompt\" not in inputs:\n            raise KeyError(\"'prompt' is a required arguments\")\n\n        inputs[\"max_iterations\"] = inputs.get(\"max_iterations\", 3)\n        inputs[\"current_iteration\"] = 0\n        inputs[\"agent1_solution\"] = []\n        inputs[\"agent2_critiques\"] = []\n        inputs[\"agent3_perspectives\"] = []\n        inputs[\"solution\"] = \"\"\n\n        return self._action.invoke(inputs, config)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent1_generate_solution","title":"<code>agent1_generate_solution(state)</code>","text":"<p>Agent 1: Hypothesizer.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent1_generate_solution(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"Agent 1: Hypothesizer.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Entering agent1_generate_solution. Iteration: {state['current_iteration']}\"\n    )\n\n    current_iter = state[\"current_iteration\"]\n    user_content = f\"Question: {state['question']}\\n\"\n\n    if current_iter &gt; 0:\n        user_content += (\n            f\"\\nPrevious solution: {state['agent1_solution'][-1]}\"\n        )\n        user_content += f\"\\nCritique: {state['agent2_critiques'][-1]}\"\n        user_content += (\n            f\"\\nCompetitor perspective: {state['agent3_perspectives'][-1]}\"\n        )\n        user_content += (\n            \"\\n\\n**You must explicitly list how this new solution differs from the previous solution,** \"\n            \"point by point, explaining what changes were made in response to the critique and competitor perspective.\"\n            \"\\nAfterward, provide your updated solution.\"\n        )\n    else:\n        user_content += \"Research this problem and generate a solution.\"\n\n    search_query = self.llm.invoke(\n        f\"Here is a problem description: {state['question']}. Turn it into a short query to be fed into a search engine.\"\n    ).content\n    if '\"' in search_query:\n        search_query = search_query.split('\"')[1]\n    raw_search_results = self.search_tool.invoke(search_query)\n\n    # Parse the results if possible, so we can collect URLs\n    new_state = state.copy()\n    new_state[\"question_search_query\"] = search_query\n    if \"visited_sites\" not in new_state:\n        new_state[\"visited_sites\"] = []\n\n    try:\n        if isinstance(raw_search_results, str):\n            results_list = ast.literal_eval(raw_search_results)\n        else:\n            results_list = raw_search_results\n        # Each item typically might have \"link\", \"title\", \"snippet\"\n        for item in results_list:\n            link = item.get(\"link\")\n            if link:\n                print(f\"[DEBUG] Appending visited link: {link}\")\n                new_state[\"visited_sites\"].append(link)\n    except (ValueError, SyntaxError, TypeError):\n        # If it's not valid Python syntax or something else goes wrong\n        print(\"[DEBUG] Could not parse search results as Python list.\")\n        print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n    user_content += f\"\\nSearch results: {raw_search_results}\"\n\n    # Provide a system message to define this agent's role\n    messages = [\n        SystemMessage(content=self.hypothesizer_prompt),\n        HumanMessage(content=user_content),\n    ]\n    solution = self.llm.invoke(messages)\n\n    new_state[\"agent1_solution\"].append(solution.content)\n\n    # Print the entire solution in green\n    print(\n        f\"{GREEN}[Agent1 - Hypothesizer solution]\\n{solution.content}{RESET}\"\n    )\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Exiting agent1_generate_solution.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent2_critique","title":"<code>agent2_critique(state)</code>","text":"<p>Agent 2: Critic.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent2_critique(self, state: HypothesizerState) -&gt; HypothesizerState:\n    \"\"\"Agent 2: Critic.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Entering agent2_critique.\"\n    )\n\n    solution = state[\"agent1_solution\"][-1]\n    user_content = (\n        f\"Question: {state['question']}\\n\"\n        f\"Proposed solution: {solution}\\n\"\n        \"Provide a detailed critique of this solution. Identify potential flaws, assumptions, and areas for improvement.\"\n    )\n\n    fact_check_query = f\"fact check {state['question_search_query']} solution effectiveness\"\n\n    raw_search_results = self.search_tool.invoke(fact_check_query)\n\n    # Parse the results if possible, so we can collect URLs\n    new_state = state.copy()\n    if \"visited_sites\" not in new_state:\n        new_state[\"visited_sites\"] = []\n\n    try:\n        if isinstance(raw_search_results, str):\n            results_list = ast.literal_eval(raw_search_results)\n        else:\n            results_list = raw_search_results\n        # Each item typically might have \"link\", \"title\", \"snippet\"\n        for item in results_list:\n            link = item.get(\"link\")\n            if link:\n                print(f\"[DEBUG] Appending visited link: {link}\")\n                new_state[\"visited_sites\"].append(link)\n    except (ValueError, SyntaxError, TypeError):\n        # If it's not valid Python syntax or something else goes wrong\n        print(\"[DEBUG] Could not parse search results as Python list.\")\n        print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n    fact_check_results = raw_search_results\n    user_content += f\"\\nFact check results: {fact_check_results}\"\n\n    messages = [\n        SystemMessage(content=self.critic_prompt),\n        HumanMessage(content=user_content),\n    ]\n    critique = self.llm.invoke(messages)\n\n    new_state[\"agent2_critiques\"].append(critique.content)\n\n    # Print the entire critique in blue\n    print(f\"{BLUE}[Agent2 - Critic]\\n{critique.content}{RESET}\")\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Exiting agent2_critique.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent3_competitor_perspective","title":"<code>agent3_competitor_perspective(state)</code>","text":"<p>Agent 3: Competitor/Stakeholder Simulator.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent3_competitor_perspective(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"Agent 3: Competitor/Stakeholder Simulator.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Entering agent3_competitor_perspective.\"\n    )\n\n    solution = state[\"agent1_solution\"][-1]\n    critique = state[\"agent2_critiques\"][-1]\n\n    user_content = (\n        f\"Question: {state['question']}\\n\"\n        f\"Proposed solution: {solution}\\n\"\n        f\"Critique: {critique}\\n\"\n        \"Simulate how a competitor, government agency, or other stakeholder might respond to this solution.\"\n    )\n\n    competitor_search_query = (\n        f\"competitor responses to {state['question_search_query']}\"\n    )\n\n    raw_search_results = self.search_tool.invoke(competitor_search_query)\n\n    # Parse the results if possible, so we can collect URLs\n    new_state = state.copy()\n    if \"visited_sites\" not in new_state:\n        new_state[\"visited_sites\"] = []\n\n    try:\n        if isinstance(raw_search_results, str):\n            results_list = ast.literal_eval(raw_search_results)\n        else:\n            results_list = raw_search_results\n        # Each item typically might have \"link\", \"title\", \"snippet\"\n        for item in results_list:\n            link = item.get(\"link\")\n            if link:\n                print(f\"[DEBUG] Appending visited link: {link}\")\n                new_state[\"visited_sites\"].append(link)\n    except (ValueError, SyntaxError, TypeError):\n        # If it's not valid Python syntax or something else goes wrong\n        print(\"[DEBUG] Could not parse search results as Python list.\")\n        print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n    competitor_info = raw_search_results\n    user_content += f\"\\nCompetitor information: {competitor_info}\"\n\n    messages = [\n        SystemMessage(content=self.competitor_prompt),\n        HumanMessage(content=user_content),\n    ]\n    perspective = self.llm.invoke(messages)\n\n    new_state[\"agent3_perspectives\"].append(perspective.content)\n\n    # Print the entire perspective in red\n    print(\n        f\"{RED}[Agent3 - Competitor/Stakeholder Perspective]\\n{perspective.content}{RESET}\"\n    )\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Exiting agent3_competitor_perspective.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.generate_solution","title":"<code>generate_solution(state)</code>","text":"<p>Generate the overall, refined solution based on all iterations.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def generate_solution(self, state: HypothesizerState) -&gt; HypothesizerState:\n    \"\"\"Generate the overall, refined solution based on all iterations.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Entering generate_solution.\"\n    )\n    prompt = f\"Original question: {state['question']}\\n\\n\"\n    prompt += \"Evolution of solutions:\\n\"\n\n    for i in range(state[\"max_iterations\"]):\n        prompt += f\"\\nIteration {i + 1}:\\n\"\n        prompt += f\"Solution: {state['agent1_solution'][i]}\\n\"\n        prompt += f\"Critique: {state['agent2_critiques'][i]}\\n\"\n        prompt += (\n            f\"Competitor perspective: {state['agent3_perspectives'][i]}\\n\"\n        )\n\n    prompt += \"\\nBased on this iterative process, provide the overall, refined solution.\"\n\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Generating overall solution with LLM...\"\n    )\n    solution = self.llm.invoke(prompt)\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Overall solution obtained. Preview:\",\n        solution.content[:200],\n        \"...\",\n    )\n\n    new_state = state.copy()\n    new_state[\"solution\"] = solution.content\n\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Exiting generate_solution.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.summarize_process_as_latex","title":"<code>summarize_process_as_latex(state)</code>","text":"<p>Summarize how the solution changed over time, referencing each iteration's critique and competitor perspective, then produce a final LaTeX document.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def summarize_process_as_latex(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"\n    Summarize how the solution changed over time, referencing\n    each iteration's critique and competitor perspective,\n    then produce a final LaTeX document.\n    \"\"\"\n    print(\"[DEBUG] Entering summarize_process_as_latex.\")\n    llm_model = state.get(\"llm_model\", \"unknown_model\")\n\n    # Build a single string describing the entire iterative process\n    iteration_details = \"\"\n    for i, (sol, crit, comp) in enumerate(\n        zip(\n            state[\"agent1_solution\"],\n            state[\"agent2_critiques\"],\n            state[\"agent3_perspectives\"],\n        ),\n        start=1,\n    ):\n        iteration_details += (\n            f\"\\\\subsection*{{Iteration {i}}}\\n\\n\"\n            f\"\\\\textbf{{Solution:}}\\\\\\\\\\n{sol}\\n\\n\"\n            f\"\\\\textbf{{Critique:}}\\\\\\\\\\n{crit}\\n\\n\"\n            f\"\\\\textbf{{Competitor Perspective:}}\\\\\\\\\\n{comp}\\n\\n\"\n        )\n\n    # -----------------------------\n    # Write iteration_details to disk as .txt\n    # -----------------------------\n    timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    txt_filename = (\n        f\"iteration_details_{llm_model}_{timestamp_str}_chat_history.txt\"\n    )\n    with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(iteration_details)\n\n    print(f\"[DEBUG] Wrote iteration details to {txt_filename}.\")\n\n    # Prompt the LLM to produce a LaTeX doc\n    # We'll just pass it as a single string to the LLM;\n    # you could also do system+human messages if you prefer.\n    prompt = f\"\"\"\\\n        You are a system that produces a FULL LaTeX document.\n        Here is information about a multi-iteration process:\n\n        Original question: {state[\"question\"]}\n\n        Below are the solutions, critiques, and competitor perspectives from each iteration:\n\n        {iteration_details}\n\n        The solution we arrived at was:\n\n        {state[\"solution\"]}\n\n        Now produce a valid LaTeX document.  Be sure to use a table of contents.\n        It must start with an Executive Summary (that may be multiple pages) which summarizes\n        the entire iterative process.  Following that, we should include the solution in full,\n        not summarized, but reformatted for appropriate LaTeX.  And then, finally (and this will be\n        quite long), we must take all the steps - solutions, critiques, and competitor perspectives\n        and *NOT SUMMARIZE THEM* but merely reformat them for the reader.  This will be in an Appendix\n        of the full content of the steps.  Finally, include a listing of all of the websites we\n        used in our research.\n\n        You must ONLY RETURN LaTeX, nothing else.  It must be valid LaTeX syntax!\n\n        Your output should start with:\n        \\\\documentclass{{article}}\n        \\\\usepackage[margin=1in]{{geometry}}\n        etc.\n\n        It must compile without errors under pdflatex. \n    \"\"\"\n\n    # Now produce a valid LaTeX document that nicely summarizes this entire iterative process.\n    # It must include the overall solution in full, not summarized, but reformatted for appropriate\n    # LaTeX. The summarization is for the other steps.\n\n    all_visited_sites = state.get(\"visited_sites\", [])\n    # (Optional) remove duplicates by converting to a set, then back to a list\n    visited_sites_unique = list(set(all_visited_sites))\n    if visited_sites_unique:\n        websites_latex = \"\\\\section*{Websites Visited}\\\\begin{itemize}\\n\"\n        for url in visited_sites_unique:\n            print(f\"We visited: {url}\")\n            # Use \\url{} to handle special characters in URLs\n            websites_latex += f\"\\\\item \\\\url{{{url}}}\\n\"\n        websites_latex += \"\\\\end{itemize}\\n\\n\"\n    else:\n        # If no sites visited, or the list is empty\n        websites_latex = (\n            \"\\\\section*{Websites Visited}\\nNo sites were visited.\\n\\n\"\n        )\n    print(websites_latex)\n\n    # Ask the LLM to produce *only* LaTeX content\n    latex_response = self.llm.invoke(prompt)\n\n    latex_doc = latex_response.content\n\n    def inject_into_latex(original_tex: str, injection: str) -&gt; str:\n        \"\"\"\n        Find the last occurrence of '\\\\end{document}' in 'original_tex'\n        and insert 'injection' right before it.\n        If '\\\\end{document}' is not found, just append the injection at the end.\n        \"\"\"\n        injection_index = original_tex.rfind(r\"\\end{document}\")\n        if injection_index == -1:\n            # If the LLM didn't include \\end{document}, just append\n            return original_tex + \"\\n\" + injection\n        else:\n            # Insert right before \\end{document}\n            return (\n                original_tex[:injection_index]\n                + \"\\n\"\n                + injection\n                + \"\\n\"\n                + original_tex[injection_index:]\n            )\n\n    final_latex = inject_into_latex(latex_doc, websites_latex)\n\n    new_state = state.copy()\n    new_state[\"summary_report\"] = final_latex\n\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Received LaTeX from LLM. Preview:\"\n    )\n    print(latex_response.content[:300], \"...\")\n    print(\n        f\"[iteration {state['current_iteration']} - DEBUG] Exiting summarize_process_as_latex.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.mp_agent","title":"<code>mp_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.mp_agent.MaterialsProjectAgent","title":"<code>MaterialsProjectAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/mp_agent.py</code> <pre><code>class MaterialsProjectAgent(BaseAgent):\n    def __init__(\n        self,\n        llm=\"openai/o3-mini\",\n        summarize: bool = True,\n        max_results: int = 3,\n        database_path: str = \"mp_database\",\n        summaries_path: str = \"mp_summaries\",\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.summarize = summarize\n        self.max_results = max_results\n        self.database_path = database_path\n        self.summaries_path = summaries_path\n\n        os.makedirs(self.database_path, exist_ok=True)\n        os.makedirs(self.summaries_path, exist_ok=True)\n\n        self._action = self._build_graph()\n\n    def _fetch_node(self, state: Dict) -&gt; Dict:\n        f = state[\"query\"]\n        els = f[\"elements\"]  # e.g. [\"Ga\",\"In\"]\n        bg = (f[\"band_gap_min\"], f[\"band_gap_max\"])\n        e_above_hull = (0, 0)  # only on-hull (stable)\n        mats = []\n        with MPRester() as mpr:\n            # get ALL matching materials\u2026\n            all_results = mpr.materials.summary.search(\n                elements=els,\n                band_gap=bg,\n                energy_above_hull=e_above_hull,\n                is_stable=True,  # equivalent filter\n            )\n            # \u2026then take only the first `max_results`\n            for doc in all_results[: self.max_results]:\n                mid = doc.material_id\n                data = doc.dict()\n                # cache to disk\n                path = os.path.join(self.database_path, f\"{mid}.json\")\n                if not os.path.exists(path):\n                    with open(path, \"w\") as f:\n                        json.dump(data, f, indent=2)\n                mats.append({\"material_id\": mid, \"metadata\": data})\n\n        return {**state, \"materials\": mats}\n\n    def _summarize_node(self, state: Dict) -&gt; Dict:\n        \"\"\"Summarize each material via LLM over its metadata.\"\"\"\n        # prompt template\n        prompt = ChatPromptTemplate.from_template(\"\"\"\nYou are a materials-science assistant. Given the following metadata about a material, produce a concise summary focusing on its key properties:\n\n{metadata}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        summaries = [None] * len(state[\"materials\"])\n\n        def process(i, mat):\n            mid = mat[\"material_id\"]\n            meta = mat[\"metadata\"]\n            # flatten metadata to text\n            text = \"\\n\".join(f\"{k}: {v}\" for k, v in meta.items())\n            # build or load summary\n            summary_file = os.path.join(\n                self.summaries_path, f\"{mid}_summary.txt\"\n            )\n            if os.path.exists(summary_file):\n                with open(summary_file) as f:\n                    return i, f.read()\n            # optional: vectorize &amp; retrieve, but here we just summarize full text\n            result = chain.invoke({\"metadata\": text})\n            with open(summary_file, \"w\") as f:\n                f.write(result)\n            return i, result\n\n        with ThreadPoolExecutor(\n            max_workers=min(8, len(state[\"materials\"]))\n        ) as exe:\n            futures = [\n                exe.submit(process, i, m)\n                for i, m in enumerate(state[\"materials\"])\n            ]\n            for future in tqdm(futures, desc=\"Summarizing materials\"):\n                i, summ = future.result()\n                summaries[i] = summ\n\n        return {**state, \"summaries\": summaries}\n\n    def _aggregate_node(self, state: Dict) -&gt; Dict:\n        \"\"\"Combine all summaries into a single, coherent answer.\"\"\"\n        combined = \"\\n\\n----\\n\\n\".join(\n            f\"[{i + 1}] {m['material_id']}\\n\\n{summary}\"\n            for i, (m, summary) in enumerate(\n                zip(state[\"materials\"], state[\"summaries\"])\n            )\n        )\n\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a materials informatics assistant. Below are brief summaries of several materials:\n\n        {summaries}\n\n        Answer the user\u2019s question in context:\n\n        {context}\n                \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n        final = chain.invoke({\n            \"summaries\": combined,\n            \"context\": state[\"context\"],\n        })\n        return {**state, \"final_summary\": final}\n\n    def _build_graph(self):\n        graph = StateGraph(dict)  # using plain dict for state\n        self.add_node(graph, self._fetch_node)\n        if self.summarize:\n            self.add_node(graph, self._summarize_node)\n            self.add_node(graph, self._aggregate_node)\n\n            graph.set_entry_point(\"_fetch_node\")\n            graph.add_edge(\"_fetch_node\", \"_summarize_node\")\n            graph.add_edge(\"_summarize_node\", \"_aggregate_node\")\n            graph.set_finish_point(\"_aggregate_node\")\n        else:\n            graph.set_entry_point(\"_fetch_node\")\n            graph.set_finish_point(\"_fetch_node\")\n        return graph.compile(checkpointer=self.checkpointer)\n\n    def _invoke(\n        self,\n        inputs: Mapping[str, Any],\n        *,\n        summarize: bool | None = None,\n        recursion_limit: int = 1000,\n        **_,\n    ) -&gt; str:\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n\n        if \"query\" not in inputs:\n            if \"mp_query\" in inputs:\n                # make a shallow copy and rename the key\n                inputs = dict(inputs)\n                inputs[\"query\"] = inputs.pop(\"mp_query\")\n            else:\n                raise KeyError(\n                    \"Missing 'query' in inputs (alias 'mp_query' also accepted).\"\n                )\n\n        result = self._action.invoke(inputs, config)\n\n        use_summary = self.summarize if summarize is None else summarize\n        return (\n            result.get(\"final_summary\", \"No summary generated.\")\n            if use_summary\n            else \"\\n\\nFinished Fetching Materials Database Information!\"\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.optimization_agent","title":"<code>optimization_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.optimization_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run a commandline command from using the subprocess package in python</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>commandline command to be run as a string given to the subprocess.run command.</p> required Source code in <code>src/ursa/agents/optimization_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"\n    Run a commandline command from using the subprocess package in python\n\n    Args:\n        query: commandline command to be run as a string given to the subprocess.run command.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"RUNNING: \", query)\n    try:\n        process = subprocess.Popen(\n            query.split(\" \"),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            cwd=workspace_dir,\n        )\n\n        stdout, stderr = process.communicate(timeout=60000)\n    except KeyboardInterrupt:\n        print(\"Keyboard Interrupt of command: \", query)\n        stdout, stderr = \"\", \"KeyboardInterrupt:\"\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.optimization_agent.write_code","title":"<code>write_code(code, filename, state)</code>","text":"<p>Writes python or Julia code to a file in the given workspace as requested.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to write</p> required <code>filename</code> <code>str</code> <p>the filename with an appropriate extension for programming language (.py for python, .jl for Julia, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Execution results</p> Source code in <code>src/ursa/agents/optimization_agent.py</code> <pre><code>@tool\ndef write_code(\n    code: str, filename: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Writes python or Julia code to a file in the given workspace as requested.\n\n    Args:\n        code: The code to write\n        filename: the filename with an appropriate extension for programming language (.py for python, .jl for Julia, etc.)\n\n    Returns:\n        Execution results\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"Writing filename \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.websearch_agent","title":"<code>websearch_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.websearch_agent.WebSearchAgent","title":"<code>WebSearchAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/websearch_agent.py</code> <pre><code>class WebSearchAgent(BaseAgent):\n    def __init__(\n        self, llm: str | BaseChatModel = \"openai/gpt-4o-mini\", **kwargs\n    ):\n        super().__init__(llm, **kwargs)\n        self.websearch_prompt = websearch_prompt\n        self.reflection_prompt = reflection_prompt\n        self.tools = [search_tool, process_content]  # + cb_tools\n        self.has_internet = self._check_for_internet(\n            kwargs.get(\"url\", \"http://www.lanl.gov\")\n        )\n        self._build_graph()\n\n    def _review_node(self, state: WebSearchState) -&gt; WebSearchState:\n        if not self.has_internet:\n            return {\n                \"messages\": [\n                    HumanMessage(\n                        content=\"No internet for WebSearch Agent so no research to review.\"\n                    )\n                ],\n                \"urls_visited\": [],\n            }\n\n        translated = [SystemMessage(content=reflection_prompt)] + state[\n            \"messages\"\n        ]\n        res = self.llm.invoke(\n            translated, {\"configurable\": {\"thread_id\": self.thread_id}}\n        )\n        return {\"messages\": [HumanMessage(content=res.content)]}\n\n    def _response_node(self, state: WebSearchState) -&gt; WebSearchState:\n        if not self.has_internet:\n            return {\n                \"messages\": [\n                    HumanMessage(\n                        content=\"No internet for WebSearch Agent. No research carried out.\"\n                    )\n                ],\n                \"urls_visited\": [],\n            }\n\n        messages = state[\"messages\"] + [SystemMessage(content=summarize_prompt)]\n        response = self.llm.invoke(\n            messages, {\"configurable\": {\"thread_id\": self.thread_id}}\n        )\n\n        urls_visited = []\n        for message in messages:\n            if message.model_dump().get(\"tool_calls\", []):\n                if \"url\" in message.tool_calls[0][\"args\"]:\n                    urls_visited.append(message.tool_calls[0][\"args\"][\"url\"])\n        return {\"messages\": [response.content], \"urls_visited\": urls_visited}\n\n    def _check_for_internet(self, url, timeout=2):\n        \"\"\"\n        Checks for internet connectivity by attempting an HTTP GET request.\n        \"\"\"\n        try:\n            requests.get(url, timeout=timeout)\n            return True\n        except (requests.ConnectionError, requests.Timeout):\n            return False\n\n    def _state_store_node(self, state: WebSearchState) -&gt; WebSearchState:\n        state[\"thread_id\"] = self.thread_id\n        return state\n        # return dict(**state, thread_id=self.thread_id)\n\n    def _create_react(self, state: WebSearchState) -&gt; WebSearchState:\n        react_agent = create_react_agent(\n            self.llm,\n            self.tools,\n            state_schema=WebSearchState,\n            prompt=self.websearch_prompt,\n        )\n        return react_agent.invoke(state)\n\n    def _build_graph(self):\n        graph = StateGraph(WebSearchState)\n        self.add_node(graph, self._state_store_node)\n        self.add_node(graph, self._create_react)\n        self.add_node(graph, self._review_node)\n        self.add_node(graph, self._response_node)\n\n        graph.set_entry_point(\"_state_store_node\")\n        graph.add_edge(\"_state_store_node\", \"_create_react\")\n        graph.add_edge(\"_create_react\", \"_review_node\")\n        graph.set_finish_point(\"_response_node\")\n\n        graph.add_conditional_edges(\n            \"_review_node\",\n            should_continue,\n            {\n                \"_create_react\": \"_create_react\",\n                \"_response_node\": \"_response_node\",\n            },\n        )\n        self._action = graph.compile(checkpointer=self.checkpointer)\n        # self._action.get_graph().draw_mermaid_png(output_file_path=\"./websearch_agent_graph.png\", draw_method=MermaidDrawMethod.PYPPETEER)\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 1000, **_\n    ):\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n        return self._action.invoke(inputs, config)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.websearch_agent.process_content","title":"<code>process_content(url, context, state)</code>","text":"<p>Processes content from a given webpage.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>string with the url to obtain text content from.</p> required <code>context</code> <code>str</code> <p>string summary of the information the agent wants from the url for summarizing salient information.</p> required Source code in <code>src/ursa/agents/websearch_agent.py</code> <pre><code>def process_content(\n    url: str, context: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Processes content from a given webpage.\n\n    Args:\n        url: string with the url to obtain text content from.\n        context: string summary of the information the agent wants from the url for summarizing salient information.\n    \"\"\"\n    print(\"Parsing information from \", url)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    content_prompt = f\"\"\"\n    Here is the full content:\n    {soup.get_text()}\n\n    Carefully summarize the content in full detail, given the following context:\n    {context}\n    \"\"\"\n    summarized_information = (\n        state[\"model\"]\n        .invoke(\n            content_prompt, {\"configurable\": {\"thread_id\": state[\"thread_id\"]}}\n        )\n        .content\n    )\n    return summarized_information\n</code></pre>"},{"location":"api_reference/prompt_library/","title":"prompt_library","text":""},{"location":"api_reference/tools/","title":"tools","text":""},{"location":"api_reference/tools/#ursa.tools.feasibility_checker","title":"<code>feasibility_checker</code>","text":""},{"location":"api_reference/tools/#ursa.tools.feasibility_checker.heuristic_feasibility_check","title":"<code>heuristic_feasibility_check(constraints, variable_name, variable_type, variable_bounds, samples=10000)</code>","text":"<p>A tool for checking feasibility of the constraints.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Annotated[List[str], \"List of strings like 'x0+x1&lt;=5'\"]</code> <p>list of strings like 'x0 + x1 &lt;= 5', etc.</p> required <code>variable_name</code> <code>Annotated[List[str], \"List of strings like 'x0', 'x1', etc.\"]</code> <p>list of strings containing variable names used in constraint expressions.</p> required <code>variable_type</code> <code>Annotated[List[str], \"List of strings like 'real', 'integer', 'boolean', etc.\"]</code> <p>list of strings like 'real', 'integer', 'boolean', etc.</p> required <code>variable_bounds</code> <code>Annotated[List[List[float]], \"List of (lower bound, upper bound) tuples for x0, x1, ...'\"]</code> <p>list of (lower, upper) tuples for x0, x1, etc.</p> required <code>samples</code> <code>Annotated[int, 'Number of random sample. Default 10000']</code> <p>number of random samples, default value 10000</p> <code>10000</code> <p>Returns:</p> Type Description <code>Tuple[str]</code> <p>A string indicating whether a feasible solution was found.</p> Source code in <code>src/ursa/tools/feasibility_checker.py</code> <pre><code>@tool(parse_docstring=True)\ndef heuristic_feasibility_check(\n    constraints: Annotated[List[str], \"List of strings like 'x0+x1&lt;=5'\"],\n    variable_name: Annotated[\n        List[str], \"List of strings like 'x0', 'x1', etc.\"\n    ],\n    variable_type: Annotated[\n        List[str], \"List of strings like 'real', 'integer', 'boolean', etc.\"\n    ],\n    variable_bounds: Annotated[\n        List[List[float]],\n        \"List of (lower bound, upper bound) tuples for x0, x1, ...'\",\n    ],\n    samples: Annotated[int, \"Number of random sample. Default 10000\"] = 10000,\n) -&gt; Tuple[str]:\n    \"\"\"\n    A tool for checking feasibility of the constraints.\n\n    Args:\n        constraints: list of strings like 'x0 + x1 &lt;= 5', etc.\n        variable_name: list of strings containing variable names used in constraint expressions.\n        variable_type: list of strings like 'real', 'integer', 'boolean', etc.\n        variable_bounds: list of (lower, upper) tuples for x0, x1, etc.\n        samples: number of random samples, default value 10000\n\n    Returns:\n        A string indicating whether a feasible solution was found.\n    \"\"\"\n\n    symbols = sp.symbols(variable_name)\n\n    # Build a dict mapping each name to its Symbol, for parsing\n    locals_map = {name: sym for name, sym in zip(variable_name, symbols)}\n\n    # Parse constraints into Sympy Boolean expressions\n    parsed_constraints = []\n    try:\n        for expr in constraints:\n            parsed = parse_expr(\n                expr,\n                local_dict=locals_map,\n                transformations=standard_transformations,\n                evaluate=False,\n            )\n            parsed_constraints.append(parsed)\n    except Exception as e:\n        return f\"Error parsing constraints: {e}\"\n\n    # Sampling loop\n    n = len(parsed_constraints)\n    funcs = [\n        sp.lambdify(symbols, c, modules=[\"math\", \"numpy\"])\n        for c in parsed_constraints\n    ]\n    constraint_satisfied = np.zeros(n, dtype=int)\n    for _ in range(samples):\n        point = {}\n        for i, sym in enumerate(symbols):\n            typ = variable_type[i].lower()\n            low, high = variable_bounds[i]\n            if typ == \"integer\":\n                value = random.randint(int(low), int(high))\n            elif typ in (\"real\", \"continuous\"):\n                value = random.uniform(low, high)\n            elif typ in (\"boolean\", \"logical\"):\n                value = random.choice([False, True])\n            else:\n                raise ValueError(\n                    f\"Unknown type {variable_type[i]} for variable {variable_name[i]}\"\n                )\n            point[sym] = value\n\n        # Evaluate all constraints at this point\n        try:\n            vals = [point[s] for s in symbols]\n            cons_satisfaction = [\n                bool(np.asarray(f(*vals)).all()) for f in funcs\n            ]\n            if all(cons_satisfaction):\n                # Found a feasible point\n                readable = {str(k): round(v, 3) for k, v in point.items()}\n                return f\"Feasible solution found: {readable}\"\n            else:\n                constraint_satisfied += np.array(cons_satisfaction)\n        except Exception as e:\n            return f\"Error evaluating constraint at point {point}: {e}\"\n\n    rates = constraint_satisfied / samples  # fraction satisfied per constraint\n    order = np.argsort(rates)  # lowest (most violated) first\n\n    lines = []\n    for rank, idx in enumerate(order, start=1):\n        expr_text = constraints[\n            idx\n        ]  # use the original string; easier to read than str(sympy_expr)\n        sat = constraint_satisfied[idx]\n        lines.append(\n            f\"[C{idx + 1}] {expr_text} \u2014 satisfied {sat:,}/{samples:,} ({sat / samples:.1%}), \"\n            f\"violated {1 - sat / samples:.1%}\"\n        )\n\n    return (\n        f\"No feasible solution found after {samples:,} samples. Most violated constraints (low\u2192high satisfaction):\\n \"\n        + \"\\n  \".join(lines)\n    )\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.feasibility_tools","title":"<code>feasibility_tools</code>","text":"<p>Unified feasibility checker with heuristic pre-check and exact auto-routing.</p> <p>Backends (imported lazily and used only if available): - PySMT (cvc5/msat/yices/z3) for SMT-style logic, disjunctions, and nonlinear constructs. - OR-Tools CP-SAT for strictly linear integer/boolean instances with integer coefficients. - OR-Tools CBC (pywraplp) for linear MILP/LP (mixed real + integer, or pure LP). - SciPy HiGHS (linprog) for pure continuous LP feasibility.</p> Install any subset you need <p>pip install pysmt &amp;&amp; pysmt-install --cvc5        # or --z3/--msat/--yices pip install ortools pip install scipy pip install numpy</p> <p>This file exposes a single LangChain tool: <code>feasibility_check_auto</code>.</p>"},{"location":"api_reference/tools/#ursa.tools.feasibility_tools.feasibility_check_auto","title":"<code>feasibility_check_auto(constraints, variable_name, variable_type, variable_bounds, prefer_smt_solver='cvc5', heuristic_enabled=True, heuristic_first=True, heuristic_samples=2000, heuristic_seed=None, heuristic_unbounded_radius_real=1000.0, heuristic_unbounded_radius_int=10 ** 6, numeric_tolerance=1e-08)</code>","text":"<p>Unified feasibility checker with heuristic pre-check and exact auto-routing.</p> <p>Performs an optional randomized feasibility search. If no witness is found (or the heuristic is disabled), the function auto-routes to an exact backend based on the detected problem structure (PySMT for SMT/logic/nonlinear, OR-Tools CP-SAT for linear integer/boolean, OR-Tools CBC for MILP/LP, or SciPy HiGHS for pure LP).</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Annotated[List[str], \"Constraint strings like 'x0 + 2*x1 &lt;= 5' or '(x0&lt;=3) | (x1&gt;=2)'\"]</code> <p>Constraint strings such as \"x0 + 2*x1 &lt;= 5\" or \"(x0&lt;=3) | (x1&gt;=2)\".</p> required <code>variable_name</code> <code>Annotated[List[str], ['x0', 'x1', ...]]</code> <p>Variable names, e.g., [\"x0\", \"x1\"].</p> required <code>variable_type</code> <code>Annotated[List[str], ['real' | 'integer' | 'boolean', ...]]</code> <p>Variable types aligned with <code>variable_name</code>. Each must be one of \"real\", \"integer\", or \"boolean\".</p> required <code>variable_bounds</code> <code>Annotated[List[List[Optional[float]]], '[(low, high), ...] (use None for unbounded)']</code> <p>Per-variable [low, high] bounds aligned with <code>variable_name</code>. Use None to denote an unbounded side.</p> required <code>prefer_smt_solver</code> <code>Annotated[str, \"SMT backend if needed: 'cvc5'|'msat'|'yices'|'z3'\"]</code> <p>SMT backend name used by PySMT (\"cvc5\", \"msat\", \"yices\", or \"z3\").</p> <code>'cvc5'</code> <code>heuristic_enabled</code> <code>Annotated[bool, 'Run a fast randomized search first?']</code> <p>Whether to run the heuristic sampler.</p> <code>True</code> <code>heuristic_first</code> <code>Annotated[bool, 'Try heuristic before exact routing']</code> <p>If True, run the heuristic before exact routing; if False, run it after.</p> <code>True</code> <code>heuristic_samples</code> <code>Annotated[int, 'Samples for heuristic search']</code> <p>Number of heuristic samples.</p> <code>2000</code> <code>heuristic_seed</code> <code>Annotated[Optional[int], 'Seed for reproducibility']</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>heuristic_unbounded_radius_real</code> <code>Annotated[float, 'Sampling range for unbounded real vars']</code> <p>Sampling radius for unbounded real variables.</p> <code>1000.0</code> <code>heuristic_unbounded_radius_int</code> <code>Annotated[int, 'Sampling range for unbounded integer vars']</code> <p>Sampling radius for unbounded integer variables.</p> <code>10 ** 6</code> <code>numeric_tolerance</code> <code>Annotated[float, 'Tolerance for relational checks (Eq/Lt/Le/etc.)']</code> <p>Tolerance used in relational checks (e.g., Eq, Lt, Le).</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>str</code> <p>A message indicating the chosen backend and the feasibility result. On success,</p> <code>str</code> <p>includes an example model (assignment). On infeasibility, includes a short</p> <code>str</code> <p>diagnostic or solver status.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If constraints cannot be parsed or an unsupported variable type is provided.</p> Source code in <code>src/ursa/tools/feasibility_tools.py</code> <pre><code>@tool(parse_docstring=True)\ndef feasibility_check_auto(\n    constraints: Annotated[\n        List[str],\n        \"Constraint strings like 'x0 + 2*x1 &lt;= 5' or '(x0&lt;=3) | (x1&gt;=2)'\",\n    ],\n    variable_name: Annotated[List[str], \"['x0','x1',...]\"],\n    variable_type: Annotated[List[str], \"['real'|'integer'|'boolean', ...]\"],\n    variable_bounds: Annotated[\n        List[List[Optional[float]]],\n        \"[(low, high), ...] (use None for unbounded)\",\n    ],\n    prefer_smt_solver: Annotated[\n        str, \"SMT backend if needed: 'cvc5'|'msat'|'yices'|'z3'\"\n    ] = \"cvc5\",\n    heuristic_enabled: Annotated[\n        bool, \"Run a fast randomized search first?\"\n    ] = True,\n    heuristic_first: Annotated[\n        bool, \"Try heuristic before exact routing\"\n    ] = True,\n    heuristic_samples: Annotated[int, \"Samples for heuristic search\"] = 2000,\n    heuristic_seed: Annotated[Optional[int], \"Seed for reproducibility\"] = None,\n    heuristic_unbounded_radius_real: Annotated[\n        float, \"Sampling range for unbounded real vars\"\n    ] = 1e3,\n    heuristic_unbounded_radius_int: Annotated[\n        int, \"Sampling range for unbounded integer vars\"\n    ] = 10**6,\n    numeric_tolerance: Annotated[\n        float, \"Tolerance for relational checks (Eq/Lt/Le/etc.)\"\n    ] = 1e-8,\n) -&gt; str:\n    \"\"\"Unified feasibility checker with heuristic pre-check and exact auto-routing.\n\n    Performs an optional randomized feasibility search. If no witness is found (or the\n    heuristic is disabled), the function auto-routes to an exact backend based on the\n    detected problem structure (PySMT for SMT/logic/nonlinear, OR-Tools CP-SAT for\n    linear integer/boolean, OR-Tools CBC for MILP/LP, or SciPy HiGHS for pure LP).\n\n    Args:\n        constraints: Constraint strings such as \"x0 + 2*x1 &lt;= 5\" or \"(x0&lt;=3) | (x1&gt;=2)\".\n        variable_name: Variable names, e.g., [\"x0\", \"x1\"].\n        variable_type: Variable types aligned with `variable_name`. Each must be one of\n            \"real\", \"integer\", or \"boolean\".\n        variable_bounds: Per-variable [low, high] bounds aligned with `variable_name`.\n            Use None to denote an unbounded side.\n        prefer_smt_solver: SMT backend name used by PySMT (\"cvc5\", \"msat\", \"yices\", or \"z3\").\n        heuristic_enabled: Whether to run the heuristic sampler.\n        heuristic_first: If True, run the heuristic before exact routing; if False, run it after.\n        heuristic_samples: Number of heuristic samples.\n        heuristic_seed: Random seed for reproducibility.\n        heuristic_unbounded_radius_real: Sampling radius for unbounded real variables.\n        heuristic_unbounded_radius_int: Sampling radius for unbounded integer variables.\n        numeric_tolerance: Tolerance used in relational checks (e.g., Eq, Lt, Le).\n\n    Returns:\n        A message indicating the chosen backend and the feasibility result. On success,\n        includes an example model (assignment). On infeasibility, includes a short\n        diagnostic or solver status.\n\n    Raises:\n        ValueError: If constraints cannot be parsed or an unsupported variable type is provided.\n    \"\"\"\n    # 1) Parse\n    try:\n        symbols, sympy_cons = _parse_constraints(constraints, variable_name)\n    except Exception as e:\n        return f\"Parse error: {e}\"\n\n    # 2) Heuristic (optional)\n    if heuristic_enabled and heuristic_first:\n        try:\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        except Exception:\n            # Ignore heuristic issues and continue to exact route\n            pass\n\n    # 3) Classify &amp; route\n    info = _classify(sympy_cons, symbols, variable_type)\n\n    # SMT needed or nonlinear / non-conj\n    if info[\"requires_smt\"] or not info[\"all_linear\"]:\n        res = _solve_with_pysmt(\n            sympy_cons,\n            symbols,\n            variable_name,\n            variable_type,\n            variable_bounds,\n            solver_name=prefer_smt_solver,\n        )\n        # Optional heuristic after exact if requested\n        if (\n            heuristic_enabled\n            and not heuristic_first\n            and any(\n                kw in res.lower()\n                for kw in (\"unknown\", \"not installed\", \"unsupported\", \"failed\")\n            )\n        ):\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        return res\n\n    # Linear-only path: collect atomic conjuncts\n    conjuncts: List[sp.Expr] = []\n    for c in sympy_cons:\n        atoms, _ = _flatten_conjunction(c)\n        conjuncts.extend(atoms)\n\n    has_int, has_bool, has_real = (\n        info[\"has_int\"],\n        info[\"has_bool\"],\n        info[\"has_real\"],\n    )\n\n    # Pure LP (continuous only)\n    if not has_int and not has_bool and has_real:\n        res = _solve_with_highs_lp(\n            conjuncts, symbols, variable_name, variable_bounds\n        )\n        if \"not installed\" in res.lower():\n            res = _solve_with_cbc_milp(\n                conjuncts,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n            )\n        if (\n            heuristic_enabled\n            and not heuristic_first\n            and any(kw in res.lower() for kw in (\"failed\", \"unknown\"))\n        ):\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        return res\n\n    # All integer/boolean \u2192 CP-SAT first (if integer coefficients), else CBC MILP\n    if (has_int or has_bool) and not has_real:\n        res = _solve_with_cpsat_integer_boolean(\n            conjuncts, symbols, variable_name, variable_type, variable_bounds\n        )\n        if (\n            any(\n                kw in res\n                for kw in (\n                    \"routing to MILP/LP\",\n                    \"handles linear conjunctions only\",\n                )\n            )\n            or \"not installed\" in res.lower()\n        ):\n            res = _solve_with_cbc_milp(\n                conjuncts,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n            )\n        return res\n\n    # Mixed reals + integers \u2192 CBC MILP\n    res = _solve_with_cbc_milp(\n        conjuncts, symbols, variable_name, variable_type, variable_bounds\n    )\n\n    # Optional heuristic after exact (if backend missing/failing)\n    if (\n        heuristic_enabled\n        and not heuristic_first\n        and any(\n            kw in res.lower() for kw in (\"not installed\", \"failed\", \"status:\")\n        )\n    ):\n        h_model = _heuristic_feasible(\n            sympy_cons,\n            symbols,\n            variable_name,\n            variable_type,\n            variable_bounds,\n            samples=heuristic_samples,\n            seed=heuristic_seed,\n            tol=numeric_tolerance,\n            unbounded_radius_real=heuristic_unbounded_radius_real,\n            unbounded_radius_int=heuristic_unbounded_radius_int,\n        )\n        if h_model is not None:\n            return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n\n    return res\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.run_command","title":"<code>run_command</code>","text":""},{"location":"api_reference/tools/#ursa.tools.run_command.run_cmd","title":"<code>run_cmd(query, workspace_dir)</code>","text":"<p>Run command from commandline in the directory workspace_dir</p> Source code in <code>src/ursa/tools/run_command.py</code> <pre><code>@tool\ndef run_cmd(query: str, workspace_dir: str) -&gt; str:\n    \"\"\"Run command from commandline in the directory workspace_dir\"\"\"\n\n    print(\"RUNNING: \", query)\n    print(\n        \"DANGER DANGER DANGER - THERE IS NO GUARDRAIL FOR SAFETY IN THIS IMPLEMENTATION - DANGER DANGER DANGER\"\n    )\n    process = subprocess.Popen(\n        query.split(\" \"),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        cwd=workspace_dir,\n    )\n\n    stdout, stderr = process.communicate(timeout=600)\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.write_code","title":"<code>write_code</code>","text":""},{"location":"api_reference/tools/#ursa.tools.write_code.write_python","title":"<code>write_python(code, filename, workspace_dir)</code>","text":"<p>Writes code to a file in the given workspace.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to write</p> required <code>filename</code> <code>str</code> <p>the filename to write</p> required <p>Returns:</p> Type Description <code>str</code> <p>File writing status: string</p> Source code in <code>src/ursa/tools/write_code.py</code> <pre><code>@tool\ndef write_python(code: str, filename: str, workspace_dir: str) -&gt; str:\n    \"\"\"\n    Writes code to a file in the given workspace.\n\n    Args:\n        code: The code to write\n        filename: the filename to write\n\n    Returns:\n        File writing status: string\n    \"\"\"\n    print(\"Writing filename \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/util/","title":"util","text":""},{"location":"api_reference/util/#ursa.util.diff_renderer","title":"<code>diff_renderer</code>","text":""},{"location":"api_reference/util/#ursa.util.diff_renderer.DiffRenderer","title":"<code>DiffRenderer</code>","text":"<p>Renderable diff\u2014<code>console.print(DiffRenderer(...))</code></p> Source code in <code>src/ursa/util/diff_renderer.py</code> <pre><code>class DiffRenderer:\n    \"\"\"Renderable diff\u2014`console.print(DiffRenderer(...))`\"\"\"\n\n    def __init__(self, content: str, updated: str, filename: str):\n        # total lines in each version\n        self._old_total = len(content.splitlines())\n        self._new_total = len(updated.splitlines())\n\n        # number of digits in the largest count\n        self._num_width = len(str(max(self._old_total, self._new_total))) + 2\n\n        # get the diff\n        self._diff_lines = list(\n            difflib.unified_diff(\n                content.splitlines(),\n                updated.splitlines(),\n                fromfile=f\"{filename} (original)\",\n                tofile=f\"{filename} (modified)\",\n                lineterm=\"\",\n            )\n        )\n\n        # get syntax style\n        try:\n            self._lexer_name = Syntax.guess_lexer(filename, updated)\n        except Exception:\n            self._lexer_name = \"text\"\n\n    def __rich_console__(\n        self, console: Console, opts: ConsoleOptions\n    ) -&gt; RenderResult:\n        old_line = new_line = None\n        width = console.width\n\n        for raw in self._diff_lines:\n            # grab line numbers from hunk header\n            if m := _HUNK_RE.match(raw):\n                old_line, new_line = map(int, m.groups())\n                # build a marker\n                n = self._num_width\n                tick_col = \".\" * (n - 1)\n                indent_ticks = f\" {tick_col} {tick_col}\"\n                # pad to the indent width\n                full_indent = indent_ticks.ljust(2 * n + 3)\n                yield Text(\n                    f\"{full_indent}{raw}\".ljust(width), style=\"white on grey30\"\n                )\n                continue\n\n            # skip header lines\n            if raw.startswith((\"---\", \"+++\")):\n                continue\n\n            # split the line\n            if raw.startswith(\"+\"):\n                style = _STYLE[\"add\"]\n                code = raw[1:]\n            elif raw.startswith(\"-\"):\n                style = _STYLE[\"del\"]\n                code = raw[1:]\n            else:\n                style = _STYLE[\"ctx\"]\n                code = raw[1:] if raw.startswith(\" \") else raw\n\n            # compute line numbers\n            if raw.startswith(\"+\"):\n                old_num, new_num = None, new_line\n                new_line += 1\n            elif raw.startswith(\"-\"):\n                old_num, new_num = old_line, None\n                old_line += 1\n            else:\n                old_num, new_num = old_line, new_line\n                old_line += 1\n                new_line += 1\n\n            old_str = str(old_num) if old_num is not None else \" \"\n            new_str = str(new_num) if new_num is not None else \" \"\n\n            # Syntax-highlight the code part\n            syntax = Syntax(\n                code, self._lexer_name, line_numbers=False, word_wrap=False\n            )\n            text_code: Text = syntax.highlight(code)\n            if text_code.plain.endswith(\"\\n\"):\n                text_code = text_code[:-1]\n            # apply background\n            text_code.stylize(style.bg)\n\n            # line numbers + code\n            nums = Text(\n                f\"{old_str:&gt;{self._num_width}}{new_str:&gt;{self._num_width}} \",\n                style=f\"white {style.bg}\",\n            )\n            diff_mark = Text(style.prefix, style=f\"bright_white {style.bg}\")\n            line_text = nums + diff_mark + text_code\n\n            # pad to console width\n            pad_len = width - line_text.cell_len\n            if pad_len &gt; 0:\n                line_text.append(\" \" * pad_len, style=style.bg)\n\n            yield line_text\n</code></pre>"},{"location":"api_reference/util/#ursa.util.helperFunctions","title":"<code>helperFunctions</code>","text":""},{"location":"api_reference/util/#ursa.util.helperFunctions.run_tool_calls","title":"<code>run_tool_calls(ai_msg, tools)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ai_msg</code> <code>AIMessage</code> <p>The LLM's AIMessage containing tool calls.</p> required <code>tools</code> <code>Union[ToolRegistry, Iterable[Union[Runnable, Callable[..., Any]]]]</code> <p>Either a dict {name: tool} or an iterable of tools (must have <code>.name</code>    for mapping). Each tool can be a Runnable or a plain callable.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>List[BaseMessage]</code> <p>list[BaseMessage] to feed back to the model</p> Source code in <code>src/ursa/util/helperFunctions.py</code> <pre><code>def run_tool_calls(\n    ai_msg: AIMessage,\n    tools: Union[ToolRegistry, Iterable[Union[Runnable, Callable[..., Any]]]],\n) -&gt; List[BaseMessage]:\n    \"\"\"\n    Args:\n        ai_msg: The LLM's AIMessage containing tool calls.\n        tools: Either a dict {name: tool} or an iterable of tools (must have `.name`\n               for mapping). Each tool can be a Runnable or a plain callable.\n\n    Returns:\n        out: list[BaseMessage] to feed back to the model\n    \"\"\"\n    # Build a name-&gt;tool map\n    if isinstance(tools, dict):\n        registry: ToolRegistry = tools  # type: ignore\n    else:\n        registry = {}\n        for t in tools:\n            name = getattr(t, \"name\", None) or getattr(t, \"__name__\", None)\n            if not name:\n                raise ValueError(f\"Tool {t!r} has no discoverable name.\")\n            registry[name] = t  # type: ignore\n\n    calls = extract_tool_calls(ai_msg)\n\n    if not calls:\n        return []\n\n    out: List[BaseMessage] = []\n    for call in calls:\n        name = call.get(\"name\")\n        args = call.get(\"args\", {}) or {}\n        call_id = call.get(\"id\") or f\"call_{uuid.uuid4().hex}\"\n\n        # 1) the AIMessage that generated the call\n        out.append(ai_msg)\n\n        # 2) the ToolMessage with the execution result (or error)\n        if name not in registry:\n            content = f\"ERROR: unknown tool '{name}'.\"\n        else:\n            try:\n                result = _invoke_tool(registry[name], args)\n                content = _stringify_output(result)\n            except Exception as e:\n                content = f\"ERROR: {type(e).__name__}: {e}\"\n\n        out.append(\n            ToolMessage(content=content, tool_call_id=call_id, name=name)\n        )\n\n    return out\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger","title":"<code>memory_logger</code>","text":""},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory","title":"<code>AgentMemory</code>","text":"<p>Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory--parameters","title":"Parameters","text":"<p>path : str | Path | None     Where to keep the on-disk Chroma DB.  If None, a folder called     <code>agent_memory_db</code> is created in the package\u2019s base directory. collection_name : str     Name of the Chroma collection. embedding_model :  | None     the embedding model"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory--notes","title":"Notes","text":"<ul> <li>Requires <code>langchain-chroma</code>, and <code>chromadb</code>.</li> </ul> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>class AgentMemory:\n    \"\"\"\n    Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.\n\n    Parameters\n    ----------\n    path : str | Path | None\n        Where to keep the on-disk Chroma DB.  If *None*, a folder called\n        ``agent_memory_db`` is created in the package\u2019s base directory.\n    collection_name : str\n        Name of the Chroma collection.\n    embedding_model : &lt;TODO&gt; | None\n        the embedding model\n\n    Notes\n    -----\n    * Requires `langchain-chroma`, and `chromadb`.\n    \"\"\"\n\n    @classmethod\n    def get_db_path(cls, path: Optional[str | Path]) -&gt; Path:\n        match path:\n            case None:\n                return Path.home() / \".cache\" / \"ursa\" / \"rag\" / \"db\"\n            case str():\n                return Path(path)\n            case Path():\n                return path\n            case _:\n                raise TypeError(\n                    f\"Type of path is `{type(path)}` \"\n                    \"but `Optional[str | Path]` was expected.\"\n                )\n\n    def __init__(\n        self,\n        embedding_model,\n        path: Optional[str | Path] = None,\n        collection_name: str = \"agent_memory\",\n    ) -&gt; None:\n        self.path = self.get_db_path(path)\n        self.collection_name = collection_name\n        self.path.mkdir(parents=True, exist_ok=True)\n        self.embeddings = embedding_model\n\n        # If a DB already exists, load it; otherwise defer creation until `build_index`.\n        self.vectorstore: Optional[Chroma] = None\n        if any(self.path.iterdir()):\n            self.vectorstore = Chroma(\n                collection_name=self.collection_name,\n                embedding_function=self.embeddings,\n                persist_directory=str(self.path),\n            )\n\n    # --------------------------------------------------------------------- #\n    # \u2776 Build &amp; index a brand-new database                                   #\n    # --------------------------------------------------------------------- #\n    def build_index(\n        self,\n        chunks: Sequence[str],\n        metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Create a fresh vector store from ``chunks``.  Existing data (if any)\n        are overwritten.\n\n        Parameters\n        ----------\n        chunks : Sequence[str]\n            Text snippets (already chunked) to embed.\n        metadatas : Sequence[dict] | None\n            Optional metadata dict for each chunk, same length as ``chunks``.\n        \"\"\"\n        docs = [\n            Document(\n                page_content=text, metadata=metadatas[i] if metadatas else {}\n            )\n            for i, text in enumerate(chunks)\n        ]\n\n        # Create (or overwrite) the collection\n        self.vectorstore = Chroma.from_documents(\n            documents=docs,\n            embedding=self.embeddings,\n            collection_name=self.collection_name,\n            persist_directory=str(self.path),\n        )\n\n    # --------------------------------------------------------------------- #\n    # \u2777 Add new chunks and re-index                                          #\n    # --------------------------------------------------------------------- #\n    def add_memories(\n        self,\n        new_chunks: Sequence[str],\n        metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append new text chunks to the existing store (must call `build_index`\n        first if the DB is empty).\n\n        Raises\n        ------\n        RuntimeError\n            If the vector store is not yet initialised.\n        \"\"\"\n        if self.vectorstore is None:\n            self.build_index(new_chunks, metadatas)\n            print(\"----- Vector store initialised -----\")\n\n        docs = []\n        for i, text in enumerate(new_chunks):\n            if len(text) &gt; 0:  # only add non-empty documents\n                docs.append(\n                    Document(\n                        page_content=text,\n                        metadata=metadatas[i] if metadatas else {},\n                    )\n                )\n        self.vectorstore.add_documents(docs)\n\n    # --------------------------------------------------------------------- #\n    # \u2778 Retrieve relevant chunks (RAG query)                                 #\n    # --------------------------------------------------------------------- #\n    def retrieve(\n        self,\n        query: str,\n        k: int = 4,\n        with_scores: bool = False,\n        **search_kwargs,\n    ):\n        \"\"\"\n        Return the *k* most similar chunks for `query`.\n\n        Parameters\n        ----------\n        query : str\n            Natural-language question or statement.\n        k : int\n            How many results to return.\n        with_scores : bool\n            If True, also return similarity scores.\n        **search_kwargs\n            Extra kwargs forwarded to Chroma\u2019s ``similarity_search*`` helpers.\n\n        Returns\n        -------\n        list[Document] | list[tuple[Document, float]]\n        \"\"\"\n        if self.vectorstore is None:\n            return [\"None\"]\n\n        if with_scores:\n            return self.vectorstore.similarity_search_with_score(\n                query, k=k, **search_kwargs\n            )\n        return self.vectorstore.similarity_search(query, k=k, **search_kwargs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.add_memories","title":"<code>add_memories(new_chunks, metadatas=None)</code>","text":"<p>Append new text chunks to the existing store (must call <code>build_index</code> first if the DB is empty).</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.add_memories--raises","title":"Raises","text":"<p>RuntimeError     If the vector store is not yet initialised.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def add_memories(\n    self,\n    new_chunks: Sequence[str],\n    metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Append new text chunks to the existing store (must call `build_index`\n    first if the DB is empty).\n\n    Raises\n    ------\n    RuntimeError\n        If the vector store is not yet initialised.\n    \"\"\"\n    if self.vectorstore is None:\n        self.build_index(new_chunks, metadatas)\n        print(\"----- Vector store initialised -----\")\n\n    docs = []\n    for i, text in enumerate(new_chunks):\n        if len(text) &gt; 0:  # only add non-empty documents\n            docs.append(\n                Document(\n                    page_content=text,\n                    metadata=metadatas[i] if metadatas else {},\n                )\n            )\n    self.vectorstore.add_documents(docs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.build_index","title":"<code>build_index(chunks, metadatas=None)</code>","text":"<p>Create a fresh vector store from <code>chunks</code>.  Existing data (if any) are overwritten.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.build_index--parameters","title":"Parameters","text":"<p>chunks : Sequence[str]     Text snippets (already chunked) to embed. metadatas : Sequence[dict] | None     Optional metadata dict for each chunk, same length as <code>chunks</code>.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def build_index(\n    self,\n    chunks: Sequence[str],\n    metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Create a fresh vector store from ``chunks``.  Existing data (if any)\n    are overwritten.\n\n    Parameters\n    ----------\n    chunks : Sequence[str]\n        Text snippets (already chunked) to embed.\n    metadatas : Sequence[dict] | None\n        Optional metadata dict for each chunk, same length as ``chunks``.\n    \"\"\"\n    docs = [\n        Document(\n            page_content=text, metadata=metadatas[i] if metadatas else {}\n        )\n        for i, text in enumerate(chunks)\n    ]\n\n    # Create (or overwrite) the collection\n    self.vectorstore = Chroma.from_documents(\n        documents=docs,\n        embedding=self.embeddings,\n        collection_name=self.collection_name,\n        persist_directory=str(self.path),\n    )\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve","title":"<code>retrieve(query, k=4, with_scores=False, **search_kwargs)</code>","text":"<p>Return the k most similar chunks for <code>query</code>.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve--parameters","title":"Parameters","text":"<p>query : str     Natural-language question or statement. k : int     How many results to return. with_scores : bool     If True, also return similarity scores. **search_kwargs     Extra kwargs forwarded to Chroma\u2019s <code>similarity_search*</code> helpers.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve--returns","title":"Returns","text":"<p>list[Document] | list[tuple[Document, float]]</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def retrieve(\n    self,\n    query: str,\n    k: int = 4,\n    with_scores: bool = False,\n    **search_kwargs,\n):\n    \"\"\"\n    Return the *k* most similar chunks for `query`.\n\n    Parameters\n    ----------\n    query : str\n        Natural-language question or statement.\n    k : int\n        How many results to return.\n    with_scores : bool\n        If True, also return similarity scores.\n    **search_kwargs\n        Extra kwargs forwarded to Chroma\u2019s ``similarity_search*`` helpers.\n\n    Returns\n    -------\n    list[Document] | list[tuple[Document, float]]\n    \"\"\"\n    if self.vectorstore is None:\n        return [\"None\"]\n\n    if with_scores:\n        return self.vectorstore.similarity_search_with_score(\n            query, k=k, **search_kwargs\n        )\n    return self.vectorstore.similarity_search(query, k=k, **search_kwargs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.delete_database","title":"<code>delete_database(path=None)</code>","text":"<p>Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.delete_database--parameters","title":"Parameters","text":"<p>path : str | Path | None     Where the on-disk Chroma DB is for deleting.  If None, a folder called     <code>agent_memory_db</code> is created in the package\u2019s base directory.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def delete_database(path: Optional[str | Path] = None):\n    \"\"\"\n    Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.\n\n    Parameters\n    ----------\n    path : str | Path | None\n        Where the on-disk Chroma DB is for deleting.  If *None*, a folder called\n        ``agent_memory_db`` is created in the package\u2019s base directory.\n    \"\"\"\n    db_path = AgentMemory.get_db_path(path)\n    if os.path.exists(db_path):\n        shutil.rmtree(db_path)\n        print(f\"Database: {db_path} has been deleted.\")\n    else:\n        print(\"No database found to delete.\")\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse","title":"<code>parse</code>","text":""},{"location":"api_reference/util/#ursa.util.parse.extract_json","title":"<code>extract_json(text)</code>","text":"<p>Extract a JSON object or array from text that might contain markdown or other content.</p> The function attempts three strategies <ol> <li>Extract JSON from a markdown code block labeled as JSON.</li> <li>Extract JSON from any markdown code block.</li> <li>Use bracket matching to extract a JSON substring starting with '{' or '['.</li> </ol> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A Python object parsed from the JSON string (dict or list).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid JSON is found.</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def extract_json(text: str) -&gt; list[dict]:\n    \"\"\"\n    Extract a JSON object or array from text that might contain markdown or other content.\n\n    The function attempts three strategies:\n        1. Extract JSON from a markdown code block labeled as JSON.\n        2. Extract JSON from any markdown code block.\n        3. Use bracket matching to extract a JSON substring starting with '{' or '['.\n\n    Returns:\n        A Python object parsed from the JSON string (dict or list).\n\n    Raises:\n        ValueError: If no valid JSON is found.\n    \"\"\"\n    # Approach 1: Look for a markdown code block specifically labeled as JSON.\n    labeled_block = re.search(\n        r\"```json\\s*([\\[{].*?[\\]}])\\s*```\", text, re.DOTALL\n    )\n    if labeled_block:\n        json_str = labeled_block.group(1).strip()\n        try:\n            return json.loads(json_str)\n        except json.JSONDecodeError:\n            # Fall back to the next approach if parsing fails.\n            pass\n\n    # Approach 2: Look for any code block delimited by triple backticks.\n    generic_block = re.search(r\"```(.*?)```\", text, re.DOTALL)\n    if generic_block:\n        json_str = generic_block.group(1).strip()\n        if json_str.startswith(\"{\") or json_str.startswith(\"[\"):\n            try:\n                return json.loads(json_str)\n            except json.JSONDecodeError:\n                pass\n\n    # Approach 3: Attempt to extract JSON using bracket matching.\n    # Find the first occurrence of either '{' or '['.\n    first_obj = text.find(\"{\")\n    first_arr = text.find(\"[\")\n    if first_obj == -1 and first_arr == -1:\n        raise ValueError(\"No JSON object or array found in the text.\")\n\n    # Determine which bracket comes first.\n    if first_obj == -1:\n        start = first_arr\n        open_bracket = \"[\"\n        close_bracket = \"]\"\n    elif first_arr == -1:\n        start = first_obj\n        open_bracket = \"{\"\n        close_bracket = \"}\"\n    else:\n        if first_obj &lt; first_arr:\n            start = first_obj\n            open_bracket = \"{\"\n            close_bracket = \"}\"\n        else:\n            start = first_arr\n            open_bracket = \"[\"\n            close_bracket = \"]\"\n\n    # Bracket matching: find the matching closing bracket.\n    depth = 0\n    end = None\n    for i in range(start, len(text)):\n        if text[i] == open_bracket:\n            depth += 1\n        elif text[i] == close_bracket:\n            depth -= 1\n            if depth == 0:\n                end = i\n                break\n\n    if end is None:\n        raise ValueError(\n            \"Could not find matching closing bracket for JSON content.\"\n        )\n\n    json_str = text[start : end + 1]\n    try:\n        return json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Extracted content is not valid JSON.\") from e\n</code></pre>"}]}